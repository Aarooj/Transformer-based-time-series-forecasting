{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0160f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(15000, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(2)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers\n",
    "from keras import backend\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "#PARAMETERS\n",
    "#samples = 200000 #number of PDF samples\n",
    "samples = 150000\n",
    "seq_length = 25 #sequence window to be used\n",
    "batch_size = 128\n",
    "molecules = 2\n",
    "species = 'hmf'\n",
    "downsample_factor = 10\n",
    "\n",
    "#https://www.pythonfixing.com/2022/01/fixed-tesnorflow-20-tfrandomsetseed-not.html\n",
    "def reset_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(2)\n",
    "    tf.random.set_seed(28)\n",
    "    np.random.seed(28)\n",
    "    random.seed(28)\n",
    "\n",
    "#reset_random_seeds()\n",
    "\n",
    "data_1 = np.load('C:/Users/mali19/Downloads/KevinCode/inputs/input_latest/%s_data.npy' %species)\n",
    "data = np.add(data_1, 3.549)\n",
    "data = data[:samples:downsample_factor, :]\n",
    "print(np.shape(data))\n",
    "#scaler = MinMaxScaler()\n",
    "#data = scaler.fit_transform(data)\n",
    "#joblib.dump(scaler, '%s_scaler.gz' %species)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3d6c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.146, 1.063, 0.697, 1.327, 1.289, 0.783])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[180000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52b0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x = data[i:(i+seq_length), :]\n",
    "        y = data[i+seq_length, :]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "input_, output = create_sequences(data, seq_length)\n",
    "\n",
    "input_ = np.array(input_)\n",
    "output = np.array(output)\n",
    "\n",
    "indices = np.arange(len(input_))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "shuffled_data = input_[indices]\n",
    "shuffled_targets = output[indices]\n",
    "\n",
    "split_ratio = 0.80\n",
    "split_index = int(len(input_)*split_ratio)\n",
    "\n",
    "x1_train = shuffled_data[:split_index]\n",
    "y1_train = shuffled_targets[:split_index]\n",
    "x1_val = shuffled_data[split_index:]\n",
    "y1_val = shuffled_targets[split_index:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26b3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea621b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 25, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 25, 6)                0         ['input_1[0][0]']             \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 25, 6)                27654     ['tf.__operators__.add[0][0]',\n",
      " iHeadAttention)                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 25, 6)                0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 25, 6)                0         ['dropout[0][0]',             \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 25, 6)                12        ['tf.__operators__.add_1[0][0]\n",
      " Normalization)                                                     ']                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 25, 6)                3334      ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 25, 6)                0         ['sequential[0][0]',          \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 25, 6)                27654     ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 25, 6)                0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 25, 6)                0         ['sequential_1[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 25, 6)                27654     ['layer_normalization_3[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 25, 6)                0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 25, 6)                0         ['sequential_2[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 25, 6)                27654     ['layer_normalization_5[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 25, 6)                0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TF  (None, 25, 6)                0         ['sequential_3[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_8[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 6)                    0         ['layer_normalization_7[0][0]'\n",
      " SlicingOpLambda)                                                   ]                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 6)                    42        ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124090 (484.73 KB)\n",
      "Trainable params: 124090 (484.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "94/94 [==============================] - ETA: 0s - loss: 78.5876\n",
      "Epoch 1: val_loss improved from inf to 68.32247, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 157ms/step - loss: 78.5876 - val_loss: 68.3225\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 55.0469\n",
      "Epoch 2: val_loss improved from 68.32247 to 39.85721, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 147ms/step - loss: 55.0469 - val_loss: 39.8572\n",
      "Epoch 3/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 26.3117\n",
      "Epoch 3: val_loss improved from 39.85721 to 14.86799, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 152ms/step - loss: 26.3117 - val_loss: 14.8680\n",
      "Epoch 4/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 7.7839\n",
      "Epoch 4: val_loss improved from 14.86799 to 2.32227, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 15s 155ms/step - loss: 7.7839 - val_loss: 2.3223\n",
      "Epoch 5/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.9629\n",
      "Epoch 5: val_loss improved from 2.32227 to 0.60028, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 152ms/step - loss: 0.9629 - val_loss: 0.6003\n",
      "Epoch 6/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.6330\n",
      "Epoch 6: val_loss improved from 0.60028 to 0.56528, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 149ms/step - loss: 0.6330 - val_loss: 0.5653\n",
      "Epoch 7/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.5935\n",
      "Epoch 7: val_loss improved from 0.56528 to 0.52606, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 0.5935 - val_loss: 0.5261\n",
      "Epoch 8/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.5098\n",
      "Epoch 8: val_loss improved from 0.52606 to 0.39763, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 148ms/step - loss: 0.5098 - val_loss: 0.3976\n",
      "Epoch 9/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4584\n",
      "Epoch 9: val_loss improved from 0.39763 to 0.36857, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.4584 - val_loss: 0.3686\n",
      "Epoch 10/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4154\n",
      "Epoch 10: val_loss improved from 0.36857 to 0.35170, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.4154 - val_loss: 0.3517\n",
      "Epoch 11/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3783\n",
      "Epoch 11: val_loss improved from 0.35170 to 0.31782, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.3783 - val_loss: 0.3178\n",
      "Epoch 12/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3463\n",
      "Epoch 12: val_loss improved from 0.31782 to 0.28963, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.3463 - val_loss: 0.2896\n",
      "Epoch 13/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3251\n",
      "Epoch 13: val_loss did not improve from 0.28963\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.3251 - val_loss: 0.2903\n",
      "Epoch 14/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2894\n",
      "Epoch 14: val_loss improved from 0.28963 to 0.22991, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.2894 - val_loss: 0.2299\n",
      "Epoch 15/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2727\n",
      "Epoch 15: val_loss improved from 0.22991 to 0.21679, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.2727 - val_loss: 0.2168\n",
      "Epoch 16/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2629\n",
      "Epoch 16: val_loss did not improve from 0.21679\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.2629 - val_loss: 0.2253\n",
      "Epoch 17/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2586\n",
      "Epoch 17: val_loss did not improve from 0.21679\n",
      "94/94 [==============================] - 18s 194ms/step - loss: 0.2586 - val_loss: 0.2226\n",
      "Epoch 18/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2540\n",
      "Epoch 18: val_loss improved from 0.21679 to 0.21091, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.2540 - val_loss: 0.2109\n",
      "Epoch 19/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2492\n",
      "Epoch 19: val_loss did not improve from 0.21091\n",
      "94/94 [==============================] - 16s 169ms/step - loss: 0.2492 - val_loss: 0.2213\n",
      "Epoch 20/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2442\n",
      "Epoch 20: val_loss improved from 0.21091 to 0.20544, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.2442 - val_loss: 0.2054\n",
      "Epoch 21/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2428\n",
      "Epoch 21: val_loss did not improve from 0.20544\n",
      "94/94 [==============================] - 16s 170ms/step - loss: 0.2428 - val_loss: 0.2137\n",
      "Epoch 22/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2354\n",
      "Epoch 22: val_loss did not improve from 0.20544\n",
      "94/94 [==============================] - 16s 170ms/step - loss: 0.2354 - val_loss: 0.2086\n",
      "Epoch 23/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2344\n",
      "Epoch 23: val_loss improved from 0.20544 to 0.20366, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.2344 - val_loss: 0.2037\n",
      "Epoch 24/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2340\n",
      "Epoch 24: val_loss improved from 0.20366 to 0.18943, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 17s 177ms/step - loss: 0.2340 - val_loss: 0.1894\n",
      "Epoch 25/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2280\n",
      "Epoch 25: val_loss did not improve from 0.18943\n",
      "94/94 [==============================] - 17s 176ms/step - loss: 0.2280 - val_loss: 0.2003\n",
      "Epoch 26/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2267\n",
      "Epoch 26: val_loss did not improve from 0.18943\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 0.2267 - val_loss: 0.1967\n",
      "Epoch 27/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2218\n",
      "Epoch 27: val_loss did not improve from 0.18943\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.2218 - val_loss: 0.1896\n",
      "Epoch 28/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2253\n",
      "Epoch 28: val_loss did not improve from 0.18943\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.2253 - val_loss: 0.2037\n",
      "Epoch 29/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2134\n",
      "Epoch 29: val_loss did not improve from 0.18943\n",
      "94/94 [==============================] - 20s 216ms/step - loss: 0.2134 - val_loss: 0.2076\n",
      "Epoch 30/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2137\n",
      "Epoch 30: val_loss improved from 0.18943 to 0.18134, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 22s 236ms/step - loss: 0.2137 - val_loss: 0.1813\n",
      "Epoch 31/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2079\n",
      "Epoch 31: val_loss improved from 0.18134 to 0.17248, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 22s 233ms/step - loss: 0.2079 - val_loss: 0.1725\n",
      "Epoch 32/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2209\n",
      "Epoch 32: val_loss did not improve from 0.17248\n",
      "94/94 [==============================] - 20s 213ms/step - loss: 0.2209 - val_loss: 0.1960\n",
      "Epoch 33/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2055\n",
      "Epoch 33: val_loss did not improve from 0.17248\n",
      "94/94 [==============================] - 22s 236ms/step - loss: 0.2055 - val_loss: 0.1971\n",
      "Epoch 34/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2061\n",
      "Epoch 34: val_loss did not improve from 0.17248\n",
      "94/94 [==============================] - 20s 213ms/step - loss: 0.2061 - val_loss: 0.1899\n",
      "Epoch 35/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2009\n",
      "Epoch 35: val_loss did not improve from 0.17248\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2009 - val_loss: 0.1758\n",
      "Epoch 36/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.1997\n",
      "Epoch 36: val_loss did not improve from 0.17248\n",
      "94/94 [==============================] - 21s 219ms/step - loss: 0.1997 - val_loss: 0.1830\n",
      "Epoch 37/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1975\n",
      "Epoch 37: val_loss improved from 0.17248 to 0.16666, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.1975 - val_loss: 0.1667\n",
      "Epoch 38/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 38: val_loss improved from 0.16666 to 0.15821, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1915 - val_loss: 0.1582\n",
      "Epoch 39/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1948\n",
      "Epoch 39: val_loss did not improve from 0.15821\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1948 - val_loss: 0.1820\n",
      "Epoch 40/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1865\n",
      "Epoch 40: val_loss did not improve from 0.15821\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1865 - val_loss: 0.1685\n",
      "Epoch 41/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 41: val_loss did not improve from 0.15821\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1863 - val_loss: 0.1740\n",
      "Epoch 42/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 42: val_loss did not improve from 0.15821\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1848 - val_loss: 0.1645\n",
      "Epoch 43/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 43: val_loss improved from 0.15821 to 0.15174, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1849 - val_loss: 0.1517\n",
      "Epoch 44/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1793\n",
      "Epoch 44: val_loss did not improve from 0.15174\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1793 - val_loss: 0.1664\n",
      "Epoch 45/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1770\n",
      "Epoch 45: val_loss did not improve from 0.15174\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1770 - val_loss: 0.1537\n",
      "Epoch 46/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1755\n",
      "Epoch 46: val_loss improved from 0.15174 to 0.14732, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1755 - val_loss: 0.1473\n",
      "Epoch 47/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1783\n",
      "Epoch 47: val_loss did not improve from 0.14732\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1783 - val_loss: 0.1582\n",
      "Epoch 48/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1731\n",
      "Epoch 48: val_loss did not improve from 0.14732\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1731 - val_loss: 0.1503\n",
      "Epoch 49/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1695\n",
      "Epoch 49: val_loss improved from 0.14732 to 0.13772, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1695 - val_loss: 0.1377\n",
      "Epoch 50/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1704\n",
      "Epoch 50: val_loss did not improve from 0.13772\n",
      "94/94 [==============================] - 22s 233ms/step - loss: 0.1704 - val_loss: 0.1395\n",
      "Epoch 51/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1623\n",
      "Epoch 51: val_loss did not improve from 0.13772\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.1623 - val_loss: 0.1430\n",
      "Epoch 52/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1625\n",
      "Epoch 52: val_loss did not improve from 0.13772\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1625 - val_loss: 0.1476\n",
      "Epoch 53/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1612\n",
      "Epoch 53: val_loss improved from 0.13772 to 0.13622, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 22s 236ms/step - loss: 0.1612 - val_loss: 0.1362\n",
      "Epoch 54/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1629\n",
      "Epoch 54: val_loss did not improve from 0.13622\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1629 - val_loss: 0.1561\n",
      "Epoch 55/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1587\n",
      "Epoch 55: val_loss improved from 0.13622 to 0.13388, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.1587 - val_loss: 0.1339\n",
      "Epoch 56/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1599\n",
      "Epoch 56: val_loss improved from 0.13388 to 0.12935, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 216ms/step - loss: 0.1599 - val_loss: 0.1294\n",
      "Epoch 57/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1561\n",
      "Epoch 57: val_loss did not improve from 0.12935\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.1561 - val_loss: 0.1606\n",
      "Epoch 58/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1525\n",
      "Epoch 58: val_loss did not improve from 0.12935\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.1525 - val_loss: 0.1363\n",
      "Epoch 59/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1532\n",
      "Epoch 59: val_loss did not improve from 0.12935\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.1532 - val_loss: 0.1408\n",
      "Epoch 60/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 60: val_loss did not improve from 0.12935\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.1523 - val_loss: 0.1575\n",
      "Epoch 61/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1579\n",
      "Epoch 61: val_loss improved from 0.12935 to 0.12766, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.1579 - val_loss: 0.1277\n",
      "Epoch 62/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1498\n",
      "Epoch 62: val_loss did not improve from 0.12766\n",
      "94/94 [==============================] - 20s 217ms/step - loss: 0.1498 - val_loss: 0.1538\n",
      "Epoch 63/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1501\n",
      "Epoch 63: val_loss did not improve from 0.12766\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.1501 - val_loss: 0.1312\n",
      "Epoch 64/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1462\n",
      "Epoch 64: val_loss did not improve from 0.12766\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1462 - val_loss: 0.1432\n",
      "Epoch 65/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1486\n",
      "Epoch 65: val_loss improved from 0.12766 to 0.12583, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1486 - val_loss: 0.1258\n",
      "Epoch 66/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1445\n",
      "Epoch 66: val_loss improved from 0.12583 to 0.12493, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1445 - val_loss: 0.1249\n",
      "Epoch 67/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 67: val_loss did not improve from 0.12493\n",
      "94/94 [==============================] - 21s 223ms/step - loss: 0.1423 - val_loss: 0.1310\n",
      "Epoch 68/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1422\n",
      "Epoch 68: val_loss did not improve from 0.12493\n",
      "94/94 [==============================] - 23s 241ms/step - loss: 0.1422 - val_loss: 0.1308\n",
      "Epoch 69/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1409\n",
      "Epoch 69: val_loss did not improve from 0.12493\n",
      "94/94 [==============================] - 21s 225ms/step - loss: 0.1409 - val_loss: 0.1257\n",
      "Epoch 70/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1411\n",
      "Epoch 70: val_loss improved from 0.12493 to 0.12089, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.1411 - val_loss: 0.1209\n",
      "Epoch 71/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1398\n",
      "Epoch 71: val_loss did not improve from 0.12089\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1398 - val_loss: 0.1266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1403\n",
      "Epoch 72: val_loss did not improve from 0.12089\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1403 - val_loss: 0.1340\n",
      "Epoch 73/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1398\n",
      "Epoch 73: val_loss improved from 0.12089 to 0.11915, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1398 - val_loss: 0.1192\n",
      "Epoch 74/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1347\n",
      "Epoch 74: val_loss did not improve from 0.11915\n",
      "94/94 [==============================] - 23s 245ms/step - loss: 0.1347 - val_loss: 0.1245\n",
      "Epoch 75/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1391\n",
      "Epoch 75: val_loss did not improve from 0.11915\n",
      "94/94 [==============================] - 25s 264ms/step - loss: 0.1391 - val_loss: 0.1313\n",
      "Epoch 76/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1358\n",
      "Epoch 76: val_loss improved from 0.11915 to 0.11338, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 268ms/step - loss: 0.1358 - val_loss: 0.1134\n",
      "Epoch 77/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1345\n",
      "Epoch 77: val_loss did not improve from 0.11338\n",
      "94/94 [==============================] - 25s 269ms/step - loss: 0.1345 - val_loss: 0.1152\n",
      "Epoch 78/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1432\n",
      "Epoch 78: val_loss improved from 0.11338 to 0.11188, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 26s 282ms/step - loss: 0.1432 - val_loss: 0.1119\n",
      "Epoch 79/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1323\n",
      "Epoch 79: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 27s 290ms/step - loss: 0.1323 - val_loss: 0.1206\n",
      "Epoch 80/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1331\n",
      "Epoch 80: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 26s 280ms/step - loss: 0.1331 - val_loss: 0.1171\n",
      "Epoch 81/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1323\n",
      "Epoch 81: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 25s 266ms/step - loss: 0.1323 - val_loss: 0.1268\n",
      "Epoch 82/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1308\n",
      "Epoch 82: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 27s 284ms/step - loss: 0.1308 - val_loss: 0.1155\n",
      "Epoch 83/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 83: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 24s 260ms/step - loss: 0.1279 - val_loss: 0.1136\n",
      "Epoch 84/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1292\n",
      "Epoch 84: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 25s 269ms/step - loss: 0.1292 - val_loss: 0.1333\n",
      "Epoch 85/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1292\n",
      "Epoch 85: val_loss did not improve from 0.11188\n",
      "94/94 [==============================] - 28s 293ms/step - loss: 0.1292 - val_loss: 0.1327\n",
      "Epoch 86/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 86: val_loss improved from 0.11188 to 0.10885, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 271ms/step - loss: 0.1286 - val_loss: 0.1088\n",
      "Epoch 87/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 87: val_loss did not improve from 0.10885\n",
      "94/94 [==============================] - 26s 281ms/step - loss: 0.1260 - val_loss: 0.1243\n",
      "Epoch 88/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1278\n",
      "Epoch 88: val_loss improved from 0.10885 to 0.10776, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 270ms/step - loss: 0.1278 - val_loss: 0.1078\n",
      "Epoch 89/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1259\n",
      "Epoch 89: val_loss did not improve from 0.10776\n",
      "94/94 [==============================] - 26s 272ms/step - loss: 0.1259 - val_loss: 0.1109\n",
      "Epoch 90/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 90: val_loss improved from 0.10776 to 0.10557, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 264ms/step - loss: 0.1235 - val_loss: 0.1056\n",
      "Epoch 91/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 91: val_loss did not improve from 0.10557\n",
      "94/94 [==============================] - 25s 262ms/step - loss: 0.1231 - val_loss: 0.1137\n",
      "Epoch 92/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 92: val_loss improved from 0.10557 to 0.10405, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 271ms/step - loss: 0.1286 - val_loss: 0.1041\n",
      "Epoch 93/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1224\n",
      "Epoch 93: val_loss did not improve from 0.10405\n",
      "94/94 [==============================] - 26s 274ms/step - loss: 0.1224 - val_loss: 0.1082\n",
      "Epoch 94/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1251\n",
      "Epoch 94: val_loss improved from 0.10405 to 0.10362, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 29s 305ms/step - loss: 0.1251 - val_loss: 0.1036\n",
      "Epoch 95/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 95: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 25s 270ms/step - loss: 0.1212 - val_loss: 0.1055\n",
      "Epoch 96/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 96: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 25s 263ms/step - loss: 0.1229 - val_loss: 0.1136\n",
      "Epoch 97/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 97: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 25s 272ms/step - loss: 0.1201 - val_loss: 0.1142\n",
      "Epoch 98/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1197\n",
      "Epoch 98: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 25s 269ms/step - loss: 0.1197 - val_loss: 0.1210\n",
      "Epoch 99/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1283\n",
      "Epoch 99: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 24s 259ms/step - loss: 0.1283 - val_loss: 0.1197\n",
      "Epoch 100/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 100: val_loss did not improve from 0.10362\n",
      "94/94 [==============================] - 27s 286ms/step - loss: 0.1179 - val_loss: 0.1134\n",
      "Epoch 101/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 101: val_loss improved from 0.10362 to 0.10041, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 26s 280ms/step - loss: 0.1173 - val_loss: 0.1004\n",
      "Epoch 102/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 102: val_loss did not improve from 0.10041\n",
      "94/94 [==============================] - 25s 271ms/step - loss: 0.1166 - val_loss: 0.1019\n",
      "Epoch 103/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 103: val_loss did not improve from 0.10041\n",
      "94/94 [==============================] - 26s 274ms/step - loss: 0.1181 - val_loss: 0.1093\n",
      "Epoch 104/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 104: val_loss improved from 0.10041 to 0.09748, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 26s 281ms/step - loss: 0.1154 - val_loss: 0.0975\n",
      "Epoch 105/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1209\n",
      "Epoch 105: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 26s 273ms/step - loss: 0.1209 - val_loss: 0.1010\n",
      "Epoch 106/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 106: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 25s 270ms/step - loss: 0.1147 - val_loss: 0.1042\n",
      "Epoch 107/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 107: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 25s 269ms/step - loss: 0.1144 - val_loss: 0.1056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 108: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 25s 266ms/step - loss: 0.1163 - val_loss: 0.1194\n",
      "Epoch 109/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 109: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 25s 263ms/step - loss: 0.1127 - val_loss: 0.1013\n",
      "Epoch 110/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1247\n",
      "Epoch 110: val_loss did not improve from 0.09748\n",
      "94/94 [==============================] - 25s 271ms/step - loss: 0.1247 - val_loss: 0.0977\n",
      "Epoch 111/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 111: val_loss improved from 0.09748 to 0.09288, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 26s 275ms/step - loss: 0.1116 - val_loss: 0.0929\n",
      "Epoch 112/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 112: val_loss improved from 0.09288 to 0.09116, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 25s 270ms/step - loss: 0.1113 - val_loss: 0.0912\n",
      "Epoch 113/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 113: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 25s 262ms/step - loss: 0.1117 - val_loss: 0.0962\n",
      "Epoch 114/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 114: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 25s 262ms/step - loss: 0.1174 - val_loss: 0.0961\n",
      "Epoch 115/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 115: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 25s 265ms/step - loss: 0.1115 - val_loss: 0.1063\n",
      "Epoch 116/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 116: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 29s 311ms/step - loss: 0.1164 - val_loss: 0.0980\n",
      "Epoch 117/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 117: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 29s 308ms/step - loss: 0.1115 - val_loss: 0.0997\n",
      "Epoch 118/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 118: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 29s 310ms/step - loss: 0.1299 - val_loss: 0.1014\n",
      "Epoch 119/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 119: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 27s 292ms/step - loss: 0.1122 - val_loss: 0.0980\n",
      "Epoch 120/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 120: val_loss did not improve from 0.09116\n",
      "94/94 [==============================] - 26s 273ms/step - loss: 0.1107 - val_loss: 0.0954\n",
      "Epoch 121/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 121: val_loss improved from 0.09116 to 0.09008, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 31s 331ms/step - loss: 0.1068 - val_loss: 0.0901\n",
      "Epoch 122/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 122: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 29s 311ms/step - loss: 0.1074 - val_loss: 0.1004\n",
      "Epoch 123/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 123: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 28s 302ms/step - loss: 0.1116 - val_loss: 0.1064\n",
      "Epoch 124/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 124: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 26s 276ms/step - loss: 0.1121 - val_loss: 0.0975\n",
      "Epoch 125/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 125: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 28s 303ms/step - loss: 0.1074 - val_loss: 0.0907\n",
      "Epoch 126/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 126: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 25s 268ms/step - loss: 0.1106 - val_loss: 0.0952\n",
      "Epoch 127/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 127: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 26s 273ms/step - loss: 0.1056 - val_loss: 0.1008\n",
      "Epoch 128/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 128: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 25s 266ms/step - loss: 0.1068 - val_loss: 0.0974\n",
      "Epoch 129/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 129: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 25s 268ms/step - loss: 0.1070 - val_loss: 0.0934\n",
      "Epoch 130/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 130: val_loss did not improve from 0.09008\n",
      "94/94 [==============================] - 27s 287ms/step - loss: 0.1072 - val_loss: 0.0938\n",
      "Epoch 131/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 131: val_loss improved from 0.09008 to 0.08999, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 27s 289ms/step - loss: 0.1057 - val_loss: 0.0900\n",
      "Epoch 132/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 132: val_loss improved from 0.08999 to 0.08974, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 26s 277ms/step - loss: 0.1030 - val_loss: 0.0897\n",
      "Epoch 133/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 133: val_loss did not improve from 0.08974\n",
      "94/94 [==============================] - 26s 278ms/step - loss: 0.1065 - val_loss: 0.1035\n",
      "Epoch 134/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 134: val_loss did not improve from 0.08974\n",
      "94/94 [==============================] - 26s 279ms/step - loss: 0.1054 - val_loss: 0.0937\n",
      "Epoch 135/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 135: val_loss improved from 0.08974 to 0.08895, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 28s 295ms/step - loss: 0.1043 - val_loss: 0.0890\n",
      "Epoch 136/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 136: val_loss did not improve from 0.08895\n",
      "94/94 [==============================] - 28s 301ms/step - loss: 0.1055 - val_loss: 0.0894\n",
      "Epoch 137/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 137: val_loss improved from 0.08895 to 0.08371, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 30s 322ms/step - loss: 0.1033 - val_loss: 0.0837\n",
      "Epoch 138/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 138: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 274ms/step - loss: 0.1043 - val_loss: 0.0937\n",
      "Epoch 139/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 139: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 281ms/step - loss: 0.1054 - val_loss: 0.0876\n",
      "Epoch 140/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 140: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 275ms/step - loss: 0.1039 - val_loss: 0.0878\n",
      "Epoch 141/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 141: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 276ms/step - loss: 0.1021 - val_loss: 0.0923\n",
      "Epoch 142/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 142: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 279ms/step - loss: 0.1071 - val_loss: 0.0920\n",
      "Epoch 143/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 143: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 274ms/step - loss: 0.1053 - val_loss: 0.0906\n",
      "Epoch 144/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 144: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 26s 274ms/step - loss: 0.1024 - val_loss: 0.0886\n",
      "Epoch 145/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 145: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 30s 322ms/step - loss: 0.1017 - val_loss: 0.0916\n",
      "Epoch 146/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 146: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 22s 227ms/step - loss: 0.1025 - val_loss: 0.0855\n",
      "Epoch 147/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 147: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 16s 169ms/step - loss: 0.1053 - val_loss: 0.0872\n",
      "Epoch 148/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 148: val_loss did not improve from 0.08371\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0999 - val_loss: 0.0878\n",
      "Epoch 149/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 149: val_loss improved from 0.08371 to 0.08305, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 0.0996 - val_loss: 0.0831\n",
      "Epoch 150/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 150: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 23s 242ms/step - loss: 0.1061 - val_loss: 0.0866\n",
      "Epoch 151/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 151: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 0.1021 - val_loss: 0.0913\n",
      "Epoch 152/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 152: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 25s 267ms/step - loss: 0.0993 - val_loss: 0.0962\n",
      "Epoch 153/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 153: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 24s 251ms/step - loss: 0.0999 - val_loss: 0.0859\n",
      "Epoch 154/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 154: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 22s 237ms/step - loss: 0.1001 - val_loss: 0.0875\n",
      "Epoch 155/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 155: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 20s 218ms/step - loss: 0.1101 - val_loss: 0.0861\n",
      "Epoch 156/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 156: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 21s 229ms/step - loss: 0.0985 - val_loss: 0.0873\n",
      "Epoch 157/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 157: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 18s 194ms/step - loss: 0.0984 - val_loss: 0.0882\n",
      "Epoch 158/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 158: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 17s 184ms/step - loss: 0.0996 - val_loss: 0.0836\n",
      "Epoch 159/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 159: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 17s 178ms/step - loss: 0.1002 - val_loss: 0.0866\n",
      "Epoch 160/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 160: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 18s 188ms/step - loss: 0.1004 - val_loss: 0.0886\n",
      "Epoch 161/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 161: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.1011 - val_loss: 0.0973\n",
      "Epoch 162/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 162: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0986 - val_loss: 0.0936\n",
      "Epoch 163/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 163: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 18s 188ms/step - loss: 0.0984 - val_loss: 0.0954\n",
      "Epoch 164/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 164: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 18s 191ms/step - loss: 0.1078 - val_loss: 0.0845\n",
      "Epoch 165/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 165: val_loss did not improve from 0.08305\n",
      "94/94 [==============================] - 21s 221ms/step - loss: 0.0973 - val_loss: 0.0841\n",
      "Epoch 166/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 166: val_loss improved from 0.08305 to 0.08106, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 22s 233ms/step - loss: 0.0967 - val_loss: 0.0811\n",
      "Epoch 167/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0955\n",
      "Epoch 167: val_loss did not improve from 0.08106\n",
      "94/94 [==============================] - 22s 232ms/step - loss: 0.0955 - val_loss: 0.0857\n",
      "Epoch 168/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 168: val_loss did not improve from 0.08106\n",
      "94/94 [==============================] - 21s 221ms/step - loss: 0.1020 - val_loss: 0.0962\n",
      "Epoch 169/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 169: val_loss did not improve from 0.08106\n",
      "94/94 [==============================] - 21s 229ms/step - loss: 0.0970 - val_loss: 0.0822\n",
      "Epoch 170/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 170: val_loss did not improve from 0.08106\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0966 - val_loss: 0.0858\n",
      "Epoch 171/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 171: val_loss did not improve from 0.08106\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1005 - val_loss: 0.0896\n",
      "Epoch 172/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 172: val_loss improved from 0.08106 to 0.07759, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0965 - val_loss: 0.0776\n",
      "Epoch 173/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 173: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.0958 - val_loss: 0.0819\n",
      "Epoch 174/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 174: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0971 - val_loss: 0.0813\n",
      "Epoch 175/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 175: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1068 - val_loss: 0.0842\n",
      "Epoch 176/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0949\n",
      "Epoch 176: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0949 - val_loss: 0.0897\n",
      "Epoch 177/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0956\n",
      "Epoch 177: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0956 - val_loss: 0.0925\n",
      "Epoch 178/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 178: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1044 - val_loss: 0.0811\n",
      "Epoch 179/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0957\n",
      "Epoch 179: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.0957 - val_loss: 0.0863\n",
      "Epoch 180/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0962\n",
      "Epoch 180: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 23s 243ms/step - loss: 0.0962 - val_loss: 0.0914\n",
      "Epoch 181/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 181: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 24s 252ms/step - loss: 0.0952 - val_loss: 0.0933\n",
      "Epoch 182/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 182: val_loss did not improve from 0.07759\n",
      "94/94 [==============================] - 21s 223ms/step - loss: 0.0975 - val_loss: 0.0840\n",
      "Epoch 183/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0955\n",
      "Epoch 183: val_loss improved from 0.07759 to 0.07758, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 22s 235ms/step - loss: 0.0955 - val_loss: 0.0776\n",
      "Epoch 184/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 184: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0937 - val_loss: 0.0806\n",
      "Epoch 185/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0936\n",
      "Epoch 185: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0936 - val_loss: 0.0811\n",
      "Epoch 186/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 186: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.0944 - val_loss: 0.0788\n",
      "Epoch 187/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 187: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 21s 223ms/step - loss: 0.0975 - val_loss: 0.0911\n",
      "Epoch 188/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 188: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 21s 226ms/step - loss: 0.0977 - val_loss: 0.0927\n",
      "Epoch 189/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 189: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1077 - val_loss: 0.0896\n",
      "Epoch 190/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 190: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 218ms/step - loss: 0.0941 - val_loss: 0.0820\n",
      "Epoch 191/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 191: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 213ms/step - loss: 0.0930 - val_loss: 0.0794\n",
      "Epoch 192/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 192: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 22s 231ms/step - loss: 0.0941 - val_loss: 0.0778\n",
      "Epoch 193/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 193: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 19s 208ms/step - loss: 0.0923 - val_loss: 0.0777\n",
      "Epoch 194/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 194: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.0940 - val_loss: 0.0869\n",
      "Epoch 195/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 195: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 192ms/step - loss: 0.0987 - val_loss: 0.0779\n",
      "Epoch 196/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 196: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0932 - val_loss: 0.0782\n",
      "Epoch 197/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 197: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 0.0933 - val_loss: 0.0842\n",
      "Epoch 198/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 198: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 186ms/step - loss: 0.0927 - val_loss: 0.0902\n",
      "Epoch 199/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 199: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 186ms/step - loss: 0.0946 - val_loss: 0.0854\n",
      "Epoch 200/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 200: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 187ms/step - loss: 0.0968 - val_loss: 0.0936\n",
      "Epoch 201/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 201: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 194ms/step - loss: 0.1011 - val_loss: 0.1086\n",
      "Epoch 202/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 202: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 0.0980 - val_loss: 0.0807\n",
      "Epoch 203/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0912\n",
      "Epoch 203: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 18s 188ms/step - loss: 0.0912 - val_loss: 0.0829\n",
      "Epoch 204/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 204: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 186ms/step - loss: 0.0920 - val_loss: 0.0815\n",
      "Epoch 205/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 205: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 186ms/step - loss: 0.0934 - val_loss: 0.0887\n",
      "Epoch 206/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 206: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 17s 186ms/step - loss: 0.0930 - val_loss: 0.0805\n",
      "Epoch 207/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 207: val_loss did not improve from 0.07758\n",
      "94/94 [==============================] - 20s 217ms/step - loss: 0.0918 - val_loss: 0.0799\n",
      "Epoch 208/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 208: val_loss improved from 0.07758 to 0.07737, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.0941 - val_loss: 0.0774\n",
      "Epoch 209/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 209: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 20s 207ms/step - loss: 0.0992 - val_loss: 0.0791\n",
      "Epoch 210/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 210: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 208ms/step - loss: 0.0938 - val_loss: 0.0800\n",
      "Epoch 211/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0940\n",
      "Epoch 211: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0940 - val_loss: 0.0837\n",
      "Epoch 212/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 212: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 20s 214ms/step - loss: 0.0905 - val_loss: 0.0776\n",
      "Epoch 213/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0928\n",
      "Epoch 213: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 21s 224ms/step - loss: 0.0928 - val_loss: 0.0836\n",
      "Epoch 214/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 214: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1034 - val_loss: 0.0916\n",
      "Epoch 215/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 215: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.0952 - val_loss: 0.0837\n",
      "Epoch 216/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0919\n",
      "Epoch 216: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0919 - val_loss: 0.0807\n",
      "Epoch 217/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0924\n",
      "Epoch 217: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.0924 - val_loss: 0.0845\n",
      "Epoch 218/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 218: val_loss did not improve from 0.07737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0917 - val_loss: 0.0820\n",
      "Epoch 219/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 219: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0898 - val_loss: 0.0797\n",
      "Epoch 220/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 220: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0920 - val_loss: 0.0836\n",
      "Epoch 221/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0936\n",
      "Epoch 221: val_loss did not improve from 0.07737\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0936 - val_loss: 0.0834\n",
      "Epoch 222/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 222: val_loss improved from 0.07737 to 0.07434, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0897 - val_loss: 0.0743\n",
      "Epoch 223/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 223: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0918 - val_loss: 0.0765\n",
      "Epoch 224/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 224: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0899 - val_loss: 0.0863\n",
      "Epoch 225/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 225: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.0923 - val_loss: 0.0786\n",
      "Epoch 226/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0925\n",
      "Epoch 226: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0925 - val_loss: 0.0767\n",
      "Epoch 227/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0915\n",
      "Epoch 227: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0915 - val_loss: 0.0780\n",
      "Epoch 228/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 228: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0937 - val_loss: 0.0781\n",
      "Epoch 229/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 229: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0918 - val_loss: 0.0850\n",
      "Epoch 230/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 230: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 195ms/step - loss: 0.0941 - val_loss: 0.0831\n",
      "Epoch 231/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 231: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 208ms/step - loss: 0.1057 - val_loss: 0.0767\n",
      "Epoch 232/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 232: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.1000 - val_loss: 0.0811\n",
      "Epoch 233/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 233: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0887 - val_loss: 0.0767\n",
      "Epoch 234/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 234: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0895 - val_loss: 0.0767\n",
      "Epoch 235/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 235: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0887 - val_loss: 0.0768\n",
      "Epoch 236/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 236: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0922 - val_loss: 0.0835\n",
      "Epoch 237/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0889\n",
      "Epoch 237: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0889 - val_loss: 0.0776\n",
      "Epoch 238/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 238: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0895 - val_loss: 0.1017\n",
      "Epoch 239/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0913\n",
      "Epoch 239: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0913 - val_loss: 0.0783\n",
      "Epoch 240/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 240: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0909 - val_loss: 0.0766\n",
      "Epoch 241/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0902\n",
      "Epoch 241: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0902 - val_loss: 0.0770\n",
      "Epoch 242/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0957\n",
      "Epoch 242: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.0957 - val_loss: 0.1003\n",
      "Epoch 243/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0898\n",
      "Epoch 243: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0898 - val_loss: 0.0807\n",
      "Epoch 244/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 244: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.0905 - val_loss: 0.0808\n",
      "Epoch 245/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 245: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0914 - val_loss: 0.0856\n",
      "Epoch 246/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 246: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0896 - val_loss: 0.0942\n",
      "Epoch 247/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0909\n",
      "Epoch 247: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0909 - val_loss: 0.0779\n",
      "Epoch 248/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 248: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0979 - val_loss: 0.0760\n",
      "Epoch 249/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0889\n",
      "Epoch 249: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 197ms/step - loss: 0.0889 - val_loss: 0.0766\n",
      "Epoch 250/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 250: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.0899 - val_loss: 0.0771\n",
      "Epoch 251/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0901\n",
      "Epoch 251: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0901 - val_loss: 0.0766\n",
      "Epoch 252/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 252: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.0893 - val_loss: 0.0871\n",
      "Epoch 253/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 253: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0899 - val_loss: 0.0752\n",
      "Epoch 254/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 254: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0896 - val_loss: 0.0752\n",
      "Epoch 255/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 255: val_loss did not improve from 0.07434\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.0899 - val_loss: 0.0773\n",
      "Epoch 256/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 256: val_loss improved from 0.07434 to 0.07402, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0897 - val_loss: 0.0740\n",
      "Epoch 257/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 257: val_loss did not improve from 0.07402\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.0941 - val_loss: 0.0759\n",
      "Epoch 258/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 258: val_loss did not improve from 0.07402\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.0906 - val_loss: 0.0810\n",
      "Epoch 259/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 259: val_loss did not improve from 0.07402\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0890 - val_loss: 0.0784\n",
      "Epoch 260/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 260: val_loss improved from 0.07402 to 0.07340, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0973 - val_loss: 0.0734\n",
      "Epoch 261/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 261: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0894 - val_loss: 0.0926\n",
      "Epoch 262/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0874\n",
      "Epoch 262: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0874 - val_loss: 0.0736\n",
      "Epoch 263/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0887\n",
      "Epoch 263: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.0887 - val_loss: 0.0785\n",
      "Epoch 264/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0893\n",
      "Epoch 264: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0893 - val_loss: 0.0782\n",
      "Epoch 265/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0904\n",
      "Epoch 265: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 20s 214ms/step - loss: 0.0904 - val_loss: 0.0795\n",
      "Epoch 266/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 266: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0884 - val_loss: 0.0769\n",
      "Epoch 267/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 267: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0922 - val_loss: 0.0763\n",
      "Epoch 268/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0932\n",
      "Epoch 268: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0932 - val_loss: 0.0760\n",
      "Epoch 269/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 269: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0903 - val_loss: 0.0763\n",
      "Epoch 270/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 270: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0952 - val_loss: 0.0831\n",
      "Epoch 271/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0945\n",
      "Epoch 271: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0945 - val_loss: 0.0762\n",
      "Epoch 272/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0912\n",
      "Epoch 272: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.0912 - val_loss: 0.0760\n",
      "Epoch 273/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0918\n",
      "Epoch 273: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0918 - val_loss: 0.0868\n",
      "Epoch 274/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 274: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.0895 - val_loss: 0.0791\n",
      "Epoch 275/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0913\n",
      "Epoch 275: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0913 - val_loss: 0.0798\n",
      "Epoch 276/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 276: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0923 - val_loss: 0.0852\n",
      "Epoch 277/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0881\n",
      "Epoch 277: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.0881 - val_loss: 0.0785\n",
      "Epoch 278/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 278: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0885 - val_loss: 0.0754\n",
      "Epoch 279/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0896\n",
      "Epoch 279: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0896 - val_loss: 0.0774\n",
      "Epoch 280/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0879\n",
      "Epoch 280: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.0879 - val_loss: 0.0738\n",
      "Epoch 281/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0891\n",
      "Epoch 281: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0891 - val_loss: 0.1017\n",
      "Epoch 282/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0913\n",
      "Epoch 282: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.0913 - val_loss: 0.0766\n",
      "Epoch 283/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0903\n",
      "Epoch 283: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.0903 - val_loss: 0.1024\n",
      "Epoch 284/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 284: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0888 - val_loss: 0.0737\n",
      "Epoch 285/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 285: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0888 - val_loss: 0.0850\n",
      "Epoch 286/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 286: val_loss did not improve from 0.07340\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0905 - val_loss: 0.0746\n",
      "Epoch 287/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0875\n",
      "Epoch 287: val_loss improved from 0.07340 to 0.07207, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0875 - val_loss: 0.0721\n",
      "Epoch 288/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0926\n",
      "Epoch 288: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0926 - val_loss: 0.0762\n",
      "Epoch 289/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 289: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0880 - val_loss: 0.0841\n",
      "Epoch 290/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0899\n",
      "Epoch 290: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0899 - val_loss: 0.0742\n",
      "Epoch 291/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0872\n",
      "Epoch 291: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0872 - val_loss: 0.0744\n",
      "Epoch 292/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0911\n",
      "Epoch 292: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.0911 - val_loss: 0.0761\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 293: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0884 - val_loss: 0.0818\n",
      "Epoch 294/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0875\n",
      "Epoch 294: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.0875 - val_loss: 0.0808\n",
      "Epoch 295/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 295: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.0882 - val_loss: 0.0768\n",
      "Epoch 296/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 296: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.0877 - val_loss: 0.0752\n",
      "Epoch 297/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 297: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1006 - val_loss: 0.0785\n",
      "Epoch 298/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0869\n",
      "Epoch 298: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.0869 - val_loss: 0.0798\n",
      "Epoch 299/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0888\n",
      "Epoch 299: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.0888 - val_loss: 0.0734\n",
      "Epoch 300/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0882\n",
      "Epoch 300: val_loss did not improve from 0.07207\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.0882 - val_loss: 0.0747\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(inputs.shape[-1]),\n",
    "    ])\n",
    "    x = ffn(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "    return x\n",
    "\n",
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Add positional encoding\n",
    "    position_encoding = positional_encoding(input_shape[0], input_shape[1])\n",
    "    x = x + position_encoding\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Selecting the last timestep output\n",
    "    x = x[:, -1, :]  # This slices out the last timestep\n",
    "\n",
    "    outputs = layers.Dense(molecules*3, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "num_layers = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Build the model\n",
    "model = build_transformer_model(input_shape=(seq_length, molecules*3), head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, num_layers=num_layers, dropout=dropout_rate)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mape')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=50, verbose=1, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(filepath='transformer_model_%s.h5' % species, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "history = model.fit(x1_train, y1_train, callbacks=[callback, checkpoint], batch_size=batch_size, epochs=300, verbose=1, validation_data=(x1_val, y1_val), shuffle=False)\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.to_csv('11history_%s_4.csv' %species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a3674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 25, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 25, 6)                27654     ['input_1[0][0]',             \n",
      " iHeadAttention)                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 25, 6)                0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 25, 6)                0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 25, 6)                12        ['tf.__operators__.add[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 25, 6)                3334      ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 25, 6)                0         ['sequential[0][0]',          \n",
      " OpLambda)                                                           'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 25, 6)                27654     ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 25, 6)                0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 25, 6)                0         ['sequential_1[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 25, 6)                27654     ['layer_normalization_3[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 25, 6)                0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 25, 6)                0         ['sequential_2[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 25, 6)                27654     ['layer_normalization_5[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 25, 6)                0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 25, 6)                0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_5[0][0]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 25, 6)                3334      ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 25, 6)                0         ['sequential_3[0][0]',        \n",
      " OpLambda)                                                           'layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 25, 6)                12        ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 6)                    0         ['layer_normalization_7[0][0]'\n",
      " SlicingOpLambda)                                                   ]                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 6)                    42        ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124090 (484.73 KB)\n",
      "Trainable params: 124090 (484.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "94/94 [==============================] - ETA: 0s - loss: 74.2330\n",
      "Epoch 1: val_loss improved from inf to 63.15056, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 18s 151ms/step - loss: 74.2330 - val_loss: 63.1506\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 50.4596\n",
      "Epoch 2: val_loss improved from 63.15056 to 37.66280, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 151ms/step - loss: 50.4596 - val_loss: 37.6628\n",
      "Epoch 3/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 25.4364\n",
      "Epoch 3: val_loss improved from 37.66280 to 13.37641, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 149ms/step - loss: 25.4364 - val_loss: 13.3764\n",
      "Epoch 4/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 7.0835\n",
      "Epoch 4: val_loss improved from 13.37641 to 1.71371, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 150ms/step - loss: 7.0835 - val_loss: 1.7137\n",
      "Epoch 5/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.7868\n",
      "Epoch 5: val_loss improved from 1.71371 to 0.47778, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 151ms/step - loss: 0.7868 - val_loss: 0.4778\n",
      "Epoch 6/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.5723\n",
      "Epoch 6: val_loss improved from 0.47778 to 0.45847, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 14s 151ms/step - loss: 0.5723 - val_loss: 0.4585\n",
      "Epoch 7/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.5185\n",
      "Epoch 7: val_loss improved from 0.45847 to 0.42797, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 15s 154ms/step - loss: 0.5185 - val_loss: 0.4280\n",
      "Epoch 8/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4860\n",
      "Epoch 8: val_loss did not improve from 0.42797\n",
      "94/94 [==============================] - 14s 152ms/step - loss: 0.4860 - val_loss: 0.4382\n",
      "Epoch 9/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4654\n",
      "Epoch 9: val_loss improved from 0.42797 to 0.40217, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 17s 181ms/step - loss: 0.4654 - val_loss: 0.4022\n",
      "Epoch 10/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4343\n",
      "Epoch 10: val_loss improved from 0.40217 to 0.39853, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 222ms/step - loss: 0.4343 - val_loss: 0.3985\n",
      "Epoch 11/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.4115\n",
      "Epoch 11: val_loss improved from 0.39853 to 0.37038, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.4115 - val_loss: 0.3704\n",
      "Epoch 12/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3933\n",
      "Epoch 12: val_loss improved from 0.37038 to 0.35429, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 217ms/step - loss: 0.3933 - val_loss: 0.3543\n",
      "Epoch 13/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3773\n",
      "Epoch 13: val_loss improved from 0.35429 to 0.34572, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 216ms/step - loss: 0.3773 - val_loss: 0.3457\n",
      "Epoch 14/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3626\n",
      "Epoch 14: val_loss improved from 0.34572 to 0.32689, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.3626 - val_loss: 0.3269\n",
      "Epoch 15/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3523\n",
      "Epoch 15: val_loss improved from 0.32689 to 0.31907, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 217ms/step - loss: 0.3523 - val_loss: 0.3191\n",
      "Epoch 16/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3413\n",
      "Epoch 16: val_loss did not improve from 0.31907\n",
      "94/94 [==============================] - 18s 194ms/step - loss: 0.3413 - val_loss: 0.3256\n",
      "Epoch 17/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3358\n",
      "Epoch 17: val_loss improved from 0.31907 to 0.30537, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.3358 - val_loss: 0.3054\n",
      "Epoch 18/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3234\n",
      "Epoch 18: val_loss improved from 0.30537 to 0.30348, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 170ms/step - loss: 0.3234 - val_loss: 0.3035\n",
      "Epoch 19/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3236\n",
      "Epoch 19: val_loss improved from 0.30348 to 0.30056, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.3236 - val_loss: 0.3006\n",
      "Epoch 20/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3154\n",
      "Epoch 20: val_loss improved from 0.30056 to 0.28029, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 17s 178ms/step - loss: 0.3154 - val_loss: 0.2803\n",
      "Epoch 21/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3070\n",
      "Epoch 21: val_loss did not improve from 0.28029\n",
      "94/94 [==============================] - 17s 184ms/step - loss: 0.3070 - val_loss: 0.2920\n",
      "Epoch 22/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3031\n",
      "Epoch 22: val_loss did not improve from 0.28029\n",
      "94/94 [==============================] - 18s 192ms/step - loss: 0.3031 - val_loss: 0.2962\n",
      "Epoch 23/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3042\n",
      "Epoch 23: val_loss improved from 0.28029 to 0.27494, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 17s 176ms/step - loss: 0.3042 - val_loss: 0.2749\n",
      "Epoch 24/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2986\n",
      "Epoch 24: val_loss did not improve from 0.27494\n",
      "94/94 [==============================] - 17s 181ms/step - loss: 0.2986 - val_loss: 0.2762\n",
      "Epoch 25/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2921\n",
      "Epoch 25: val_loss improved from 0.27494 to 0.27212, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 18s 195ms/step - loss: 0.2921 - val_loss: 0.2721\n",
      "Epoch 26/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2916\n",
      "Epoch 26: val_loss improved from 0.27212 to 0.26509, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 224ms/step - loss: 0.2916 - val_loss: 0.2651\n",
      "Epoch 27/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2910\n",
      "Epoch 27: val_loss did not improve from 0.26509\n",
      "94/94 [==============================] - 21s 226ms/step - loss: 0.2910 - val_loss: 0.2670\n",
      "Epoch 28/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2871\n",
      "Epoch 28: val_loss did not improve from 0.26509\n",
      "94/94 [==============================] - 21s 221ms/step - loss: 0.2871 - val_loss: 0.2731\n",
      "Epoch 29/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2855\n",
      "Epoch 29: val_loss did not improve from 0.26509\n",
      "94/94 [==============================] - 21s 218ms/step - loss: 0.2855 - val_loss: 0.2725\n",
      "Epoch 30/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2854\n",
      "Epoch 30: val_loss improved from 0.26509 to 0.26003, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.2854 - val_loss: 0.2600\n",
      "Epoch 31/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2825\n",
      "Epoch 31: val_loss improved from 0.26003 to 0.25980, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2825 - val_loss: 0.2598\n",
      "Epoch 32/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2808\n",
      "Epoch 32: val_loss did not improve from 0.25980\n",
      "94/94 [==============================] - 18s 193ms/step - loss: 0.2808 - val_loss: 0.2729\n",
      "Epoch 33/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2789\n",
      "Epoch 33: val_loss did not improve from 0.25980\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2789 - val_loss: 0.2704\n",
      "Epoch 34/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2815\n",
      "Epoch 34: val_loss improved from 0.25980 to 0.25861, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2815 - val_loss: 0.2586\n",
      "Epoch 35/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2784\n",
      "Epoch 35: val_loss improved from 0.25861 to 0.25745, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.2784 - val_loss: 0.2575\n",
      "Epoch 36/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2744\n",
      "Epoch 36: val_loss improved from 0.25745 to 0.25598, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 196ms/step - loss: 0.2744 - val_loss: 0.2560\n",
      "Epoch 37/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2750\n",
      "Epoch 37: val_loss did not improve from 0.25598\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.2750 - val_loss: 0.2681\n",
      "Epoch 38/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2734\n",
      "Epoch 38: val_loss did not improve from 0.25598\n",
      "94/94 [==============================] - 20s 214ms/step - loss: 0.2734 - val_loss: 0.2611\n",
      "Epoch 39/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2696\n",
      "Epoch 39: val_loss improved from 0.25598 to 0.25126, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.2696 - val_loss: 0.2513\n",
      "Epoch 40/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2712\n",
      "Epoch 40: val_loss did not improve from 0.25126\n",
      "94/94 [==============================] - 20s 216ms/step - loss: 0.2712 - val_loss: 0.2599\n",
      "Epoch 41/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2706\n",
      "Epoch 41: val_loss did not improve from 0.25126\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.2706 - val_loss: 0.2540\n",
      "Epoch 42/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2686\n",
      "Epoch 42: val_loss did not improve from 0.25126\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.2686 - val_loss: 0.2568\n",
      "Epoch 43/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2700\n",
      "Epoch 43: val_loss did not improve from 0.25126\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2700 - val_loss: 0.2582\n",
      "Epoch 44/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2658\n",
      "Epoch 44: val_loss improved from 0.25126 to 0.25004, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2658 - val_loss: 0.2500\n",
      "Epoch 45/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2657\n",
      "Epoch 45: val_loss did not improve from 0.25004\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2657 - val_loss: 0.2614\n",
      "Epoch 46/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2682\n",
      "Epoch 46: val_loss did not improve from 0.25004\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.2682 - val_loss: 0.2565\n",
      "Epoch 47/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2683\n",
      "Epoch 47: val_loss did not improve from 0.25004\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.2683 - val_loss: 0.2550\n",
      "Epoch 48/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2646\n",
      "Epoch 48: val_loss did not improve from 0.25004\n",
      "94/94 [==============================] - 18s 195ms/step - loss: 0.2646 - val_loss: 0.2641\n",
      "Epoch 49/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2622\n",
      "Epoch 49: val_loss improved from 0.25004 to 0.24747, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.2622 - val_loss: 0.2475\n",
      "Epoch 50/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2650\n",
      "Epoch 50: val_loss improved from 0.24747 to 0.24440, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2650 - val_loss: 0.2444\n",
      "Epoch 51/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2624\n",
      "Epoch 51: val_loss did not improve from 0.24440\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.2624 - val_loss: 0.2452\n",
      "Epoch 52/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2582\n",
      "Epoch 52: val_loss did not improve from 0.24440\n",
      "94/94 [==============================] - 18s 196ms/step - loss: 0.2582 - val_loss: 0.2604\n",
      "Epoch 53/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2614\n",
      "Epoch 53: val_loss did not improve from 0.24440\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.2614 - val_loss: 0.2461\n",
      "Epoch 54/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2574\n",
      "Epoch 54: val_loss did not improve from 0.24440\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.2574 - val_loss: 0.2458\n",
      "Epoch 55/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2570\n",
      "Epoch 55: val_loss did not improve from 0.24440\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2570 - val_loss: 0.2449\n",
      "Epoch 56/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2550\n",
      "Epoch 56: val_loss improved from 0.24440 to 0.24095, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.2550 - val_loss: 0.2409\n",
      "Epoch 57/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2540\n",
      "Epoch 57: val_loss did not improve from 0.24095\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.2540 - val_loss: 0.2410\n",
      "Epoch 58/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2530\n",
      "Epoch 58: val_loss did not improve from 0.24095\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2530 - val_loss: 0.2575\n",
      "Epoch 59/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2541\n",
      "Epoch 59: val_loss did not improve from 0.24095\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2541 - val_loss: 0.2442\n",
      "Epoch 60/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2533\n",
      "Epoch 60: val_loss improved from 0.24095 to 0.23692, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.2533 - val_loss: 0.2369\n",
      "Epoch 61/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2508\n",
      "Epoch 61: val_loss improved from 0.23692 to 0.23607, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.2508 - val_loss: 0.2361\n",
      "Epoch 62/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2506\n",
      "Epoch 62: val_loss improved from 0.23607 to 0.23354, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.2506 - val_loss: 0.2335\n",
      "Epoch 63/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2496\n",
      "Epoch 63: val_loss improved from 0.23354 to 0.23271, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2496 - val_loss: 0.2327\n",
      "Epoch 64/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2490\n",
      "Epoch 64: val_loss did not improve from 0.23271\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.2490 - val_loss: 0.2329\n",
      "Epoch 65/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2478\n",
      "Epoch 65: val_loss did not improve from 0.23271\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2478 - val_loss: 0.2365\n",
      "Epoch 66/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2466\n",
      "Epoch 66: val_loss did not improve from 0.23271\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2466 - val_loss: 0.2377\n",
      "Epoch 67/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2471\n",
      "Epoch 67: val_loss improved from 0.23271 to 0.22947, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.2471 - val_loss: 0.2295\n",
      "Epoch 68/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2451\n",
      "Epoch 68: val_loss did not improve from 0.22947\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.2451 - val_loss: 0.2315\n",
      "Epoch 69/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2433\n",
      "Epoch 69: val_loss improved from 0.22947 to 0.22846, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.2433 - val_loss: 0.2285\n",
      "Epoch 70/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2416\n",
      "Epoch 70: val_loss did not improve from 0.22846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 20s 208ms/step - loss: 0.2416 - val_loss: 0.2312\n",
      "Epoch 71/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2436\n",
      "Epoch 71: val_loss improved from 0.22846 to 0.22771, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2436 - val_loss: 0.2277\n",
      "Epoch 72/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2397\n",
      "Epoch 72: val_loss improved from 0.22771 to 0.22662, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2397 - val_loss: 0.2266\n",
      "Epoch 73/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2408\n",
      "Epoch 73: val_loss improved from 0.22662 to 0.22597, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.2408 - val_loss: 0.2260\n",
      "Epoch 74/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2391\n",
      "Epoch 74: val_loss did not improve from 0.22597\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.2391 - val_loss: 0.2281\n",
      "Epoch 75/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2366\n",
      "Epoch 75: val_loss improved from 0.22597 to 0.22075, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.2366 - val_loss: 0.2208\n",
      "Epoch 76/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2370\n",
      "Epoch 76: val_loss did not improve from 0.22075\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.2370 - val_loss: 0.2236\n",
      "Epoch 77/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2358\n",
      "Epoch 77: val_loss did not improve from 0.22075\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2358 - val_loss: 0.2273\n",
      "Epoch 78/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2367\n",
      "Epoch 78: val_loss improved from 0.22075 to 0.21830, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.2367 - val_loss: 0.2183\n",
      "Epoch 79/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2346\n",
      "Epoch 79: val_loss did not improve from 0.21830\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2346 - val_loss: 0.2195\n",
      "Epoch 80/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2333\n",
      "Epoch 80: val_loss did not improve from 0.21830\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2333 - val_loss: 0.2193\n",
      "Epoch 81/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2309\n",
      "Epoch 81: val_loss improved from 0.21830 to 0.21458, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2309 - val_loss: 0.2146\n",
      "Epoch 82/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2334\n",
      "Epoch 82: val_loss did not improve from 0.21458\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2334 - val_loss: 0.2166\n",
      "Epoch 83/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2322\n",
      "Epoch 83: val_loss did not improve from 0.21458\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2322 - val_loss: 0.2296\n",
      "Epoch 84/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2302\n",
      "Epoch 84: val_loss improved from 0.21458 to 0.21316, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2302 - val_loss: 0.2132\n",
      "Epoch 85/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2280\n",
      "Epoch 85: val_loss did not improve from 0.21316\n",
      "94/94 [==============================] - 19s 197ms/step - loss: 0.2280 - val_loss: 0.2228\n",
      "Epoch 86/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2282\n",
      "Epoch 86: val_loss improved from 0.21316 to 0.21175, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2282 - val_loss: 0.2118\n",
      "Epoch 87/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2281\n",
      "Epoch 87: val_loss did not improve from 0.21175\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2281 - val_loss: 0.2163\n",
      "Epoch 88/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2269\n",
      "Epoch 88: val_loss improved from 0.21175 to 0.21078, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2269 - val_loss: 0.2108\n",
      "Epoch 89/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2261\n",
      "Epoch 89: val_loss did not improve from 0.21078\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2261 - val_loss: 0.2112\n",
      "Epoch 90/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2252\n",
      "Epoch 90: val_loss improved from 0.21078 to 0.20781, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2252 - val_loss: 0.2078\n",
      "Epoch 91/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2250\n",
      "Epoch 91: val_loss did not improve from 0.20781\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2250 - val_loss: 0.2129\n",
      "Epoch 92/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2226\n",
      "Epoch 92: val_loss did not improve from 0.20781\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2226 - val_loss: 0.2114\n",
      "Epoch 93/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2225\n",
      "Epoch 93: val_loss improved from 0.20781 to 0.20683, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 213ms/step - loss: 0.2225 - val_loss: 0.2068\n",
      "Epoch 94/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2203\n",
      "Epoch 94: val_loss improved from 0.20683 to 0.20572, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 218ms/step - loss: 0.2203 - val_loss: 0.2057\n",
      "Epoch 95/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2199\n",
      "Epoch 95: val_loss improved from 0.20572 to 0.20369, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.2199 - val_loss: 0.2037\n",
      "Epoch 96/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2232\n",
      "Epoch 96: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 21s 219ms/step - loss: 0.2232 - val_loss: 0.2279\n",
      "Epoch 97/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2185\n",
      "Epoch 97: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.2185 - val_loss: 0.2063\n",
      "Epoch 98/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2204\n",
      "Epoch 98: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.2204 - val_loss: 0.2070\n",
      "Epoch 99/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2165\n",
      "Epoch 99: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.2165 - val_loss: 0.2096\n",
      "Epoch 100/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2144\n",
      "Epoch 100: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2144 - val_loss: 0.2152\n",
      "Epoch 101/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2145\n",
      "Epoch 101: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2145 - val_loss: 0.2080\n",
      "Epoch 102/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2180\n",
      "Epoch 102: val_loss did not improve from 0.20369\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.2180 - val_loss: 0.2106\n",
      "Epoch 103/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2136\n",
      "Epoch 103: val_loss improved from 0.20369 to 0.20061, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2136 - val_loss: 0.2006\n",
      "Epoch 104/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2129\n",
      "Epoch 104: val_loss did not improve from 0.20061\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2129 - val_loss: 0.2088\n",
      "Epoch 105/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2131\n",
      "Epoch 105: val_loss improved from 0.20061 to 0.19723, saving model to transformer_model_hmf.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 20s 215ms/step - loss: 0.2131 - val_loss: 0.1972\n",
      "Epoch 106/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2104\n",
      "Epoch 106: val_loss did not improve from 0.19723\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2104 - val_loss: 0.2110\n",
      "Epoch 107/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2122\n",
      "Epoch 107: val_loss did not improve from 0.19723\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.2122 - val_loss: 0.2025\n",
      "Epoch 108/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2111\n",
      "Epoch 108: val_loss did not improve from 0.19723\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2111 - val_loss: 0.2001\n",
      "Epoch 109/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2092\n",
      "Epoch 109: val_loss improved from 0.19723 to 0.19595, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2092 - val_loss: 0.1959\n",
      "Epoch 110/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2094\n",
      "Epoch 110: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2094 - val_loss: 0.1978\n",
      "Epoch 111/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2083\n",
      "Epoch 111: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2083 - val_loss: 0.2015\n",
      "Epoch 112/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2094\n",
      "Epoch 112: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2094 - val_loss: 0.1983\n",
      "Epoch 113/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2066\n",
      "Epoch 113: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.2066 - val_loss: 0.1977\n",
      "Epoch 114/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2079\n",
      "Epoch 114: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.2079 - val_loss: 0.1985\n",
      "Epoch 115/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2052\n",
      "Epoch 115: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2052 - val_loss: 0.1964\n",
      "Epoch 116/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2046\n",
      "Epoch 116: val_loss did not improve from 0.19595\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2046 - val_loss: 0.2068\n",
      "Epoch 117/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2039\n",
      "Epoch 117: val_loss improved from 0.19595 to 0.19163, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2039 - val_loss: 0.1916\n",
      "Epoch 118/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2070\n",
      "Epoch 118: val_loss did not improve from 0.19163\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.2070 - val_loss: 0.1952\n",
      "Epoch 119/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2053\n",
      "Epoch 119: val_loss improved from 0.19163 to 0.18941, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2053 - val_loss: 0.1894\n",
      "Epoch 120/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2039\n",
      "Epoch 120: val_loss did not improve from 0.18941\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2039 - val_loss: 0.1910\n",
      "Epoch 121/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2006\n",
      "Epoch 121: val_loss improved from 0.18941 to 0.18700, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.2006 - val_loss: 0.1870\n",
      "Epoch 122/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2017\n",
      "Epoch 122: val_loss did not improve from 0.18700\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.2017 - val_loss: 0.1939\n",
      "Epoch 123/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2014\n",
      "Epoch 123: val_loss improved from 0.18700 to 0.18576, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.2014 - val_loss: 0.1858\n",
      "Epoch 124/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1996\n",
      "Epoch 124: val_loss improved from 0.18576 to 0.18468, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1996 - val_loss: 0.1847\n",
      "Epoch 125/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 125: val_loss did not improve from 0.18468\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1987 - val_loss: 0.2005\n",
      "Epoch 126/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.2026\n",
      "Epoch 126: val_loss did not improve from 0.18468\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.2026 - val_loss: 0.1874\n",
      "Epoch 127/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1990\n",
      "Epoch 127: val_loss did not improve from 0.18468\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1990 - val_loss: 0.1859\n",
      "Epoch 128/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1972\n",
      "Epoch 128: val_loss did not improve from 0.18468\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1972 - val_loss: 0.1902\n",
      "Epoch 129/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1967\n",
      "Epoch 129: val_loss improved from 0.18468 to 0.18437, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1967 - val_loss: 0.1844\n",
      "Epoch 130/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1952\n",
      "Epoch 130: val_loss did not improve from 0.18437\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1952 - val_loss: 0.1871\n",
      "Epoch 131/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1944\n",
      "Epoch 131: val_loss did not improve from 0.18437\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1944 - val_loss: 0.1846\n",
      "Epoch 132/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1924\n",
      "Epoch 132: val_loss did not improve from 0.18437\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1924 - val_loss: 0.1908\n",
      "Epoch 133/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1932\n",
      "Epoch 133: val_loss improved from 0.18437 to 0.18081, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1932 - val_loss: 0.1808\n",
      "Epoch 134/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1908\n",
      "Epoch 134: val_loss improved from 0.18081 to 0.17907, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1908 - val_loss: 0.1791\n",
      "Epoch 135/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1925\n",
      "Epoch 135: val_loss did not improve from 0.17907\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1925 - val_loss: 0.1861\n",
      "Epoch 136/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 136: val_loss improved from 0.17907 to 0.17597, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 214ms/step - loss: 0.1917 - val_loss: 0.1760\n",
      "Epoch 137/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1899\n",
      "Epoch 137: val_loss did not improve from 0.17597\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1899 - val_loss: 0.1765\n",
      "Epoch 138/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1895\n",
      "Epoch 138: val_loss improved from 0.17597 to 0.17460, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 21s 222ms/step - loss: 0.1895 - val_loss: 0.1746\n",
      "Epoch 139/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 139: val_loss did not improve from 0.17460\n",
      "94/94 [==============================] - 21s 220ms/step - loss: 0.1879 - val_loss: 0.1772\n",
      "Epoch 140/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1875\n",
      "Epoch 140: val_loss did not improve from 0.17460\n",
      "94/94 [==============================] - 20s 215ms/step - loss: 0.1875 - val_loss: 0.1783\n",
      "Epoch 141/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 0.1893\n",
      "Epoch 141: val_loss did not improve from 0.17460\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1893 - val_loss: 0.1766\n",
      "Epoch 142/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1874\n",
      "Epoch 142: val_loss did not improve from 0.17460\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1874 - val_loss: 0.1774\n",
      "Epoch 143/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1839\n",
      "Epoch 143: val_loss improved from 0.17460 to 0.17457, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1839 - val_loss: 0.1746\n",
      "Epoch 144/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 144: val_loss did not improve from 0.17457\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1845 - val_loss: 0.1748\n",
      "Epoch 145/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1852\n",
      "Epoch 145: val_loss improved from 0.17457 to 0.17453, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1852 - val_loss: 0.1745\n",
      "Epoch 146/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1885\n",
      "Epoch 146: val_loss improved from 0.17453 to 0.17417, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1885 - val_loss: 0.1742\n",
      "Epoch 147/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1810\n",
      "Epoch 147: val_loss did not improve from 0.17417\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1810 - val_loss: 0.1763\n",
      "Epoch 148/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1815\n",
      "Epoch 148: val_loss improved from 0.17417 to 0.17010, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1815 - val_loss: 0.1701\n",
      "Epoch 149/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1805\n",
      "Epoch 149: val_loss did not improve from 0.17010\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1805 - val_loss: 0.1758\n",
      "Epoch 150/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1834\n",
      "Epoch 150: val_loss did not improve from 0.17010\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1834 - val_loss: 0.1798\n",
      "Epoch 151/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1825\n",
      "Epoch 151: val_loss improved from 0.17010 to 0.16802, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1825 - val_loss: 0.1680\n",
      "Epoch 152/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1786\n",
      "Epoch 152: val_loss improved from 0.16802 to 0.16664, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1786 - val_loss: 0.1666\n",
      "Epoch 153/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1801\n",
      "Epoch 153: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.1801 - val_loss: 0.1707\n",
      "Epoch 154/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1792\n",
      "Epoch 154: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1792 - val_loss: 0.1691\n",
      "Epoch 155/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1774\n",
      "Epoch 155: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1774 - val_loss: 0.1698\n",
      "Epoch 156/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1775\n",
      "Epoch 156: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1775 - val_loss: 0.1674\n",
      "Epoch 157/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1777\n",
      "Epoch 157: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1777 - val_loss: 0.1713\n",
      "Epoch 158/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1767\n",
      "Epoch 158: val_loss did not improve from 0.16664\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1767 - val_loss: 0.1670\n",
      "Epoch 159/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1781\n",
      "Epoch 159: val_loss improved from 0.16664 to 0.16657, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1781 - val_loss: 0.1666\n",
      "Epoch 160/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1808\n",
      "Epoch 160: val_loss did not improve from 0.16657\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.1808 - val_loss: 0.1689\n",
      "Epoch 161/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1757\n",
      "Epoch 161: val_loss improved from 0.16657 to 0.16596, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 210ms/step - loss: 0.1757 - val_loss: 0.1660\n",
      "Epoch 162/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1758\n",
      "Epoch 162: val_loss did not improve from 0.16596\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1758 - val_loss: 0.1679\n",
      "Epoch 163/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1773\n",
      "Epoch 163: val_loss improved from 0.16596 to 0.16320, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.1773 - val_loss: 0.1632\n",
      "Epoch 164/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1730\n",
      "Epoch 164: val_loss did not improve from 0.16320\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1730 - val_loss: 0.1652\n",
      "Epoch 165/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1732\n",
      "Epoch 165: val_loss did not improve from 0.16320\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1732 - val_loss: 0.1646\n",
      "Epoch 166/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1736\n",
      "Epoch 166: val_loss improved from 0.16320 to 0.16138, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.1736 - val_loss: 0.1614\n",
      "Epoch 167/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1765\n",
      "Epoch 167: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 20s 211ms/step - loss: 0.1765 - val_loss: 0.1658\n",
      "Epoch 168/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1736\n",
      "Epoch 168: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1736 - val_loss: 0.1725\n",
      "Epoch 169/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1723\n",
      "Epoch 169: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1723 - val_loss: 0.1710\n",
      "Epoch 170/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1719\n",
      "Epoch 170: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1719 - val_loss: 0.1619\n",
      "Epoch 171/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1738\n",
      "Epoch 171: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1738 - val_loss: 0.1660\n",
      "Epoch 172/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1703\n",
      "Epoch 172: val_loss did not improve from 0.16138\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1703 - val_loss: 0.1686\n",
      "Epoch 173/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1721\n",
      "Epoch 173: val_loss improved from 0.16138 to 0.16113, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1721 - val_loss: 0.1611\n",
      "Epoch 174/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1705\n",
      "Epoch 174: val_loss did not improve from 0.16113\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1705 - val_loss: 0.1708\n",
      "Epoch 175/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1710\n",
      "Epoch 175: val_loss improved from 0.16113 to 0.16107, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1710 - val_loss: 0.1611\n",
      "Epoch 176/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1699\n",
      "Epoch 176: val_loss did not improve from 0.16107\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.1699 - val_loss: 0.1676\n",
      "Epoch 177/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1701\n",
      "Epoch 177: val_loss did not improve from 0.16107\n",
      "94/94 [==============================] - 19s 198ms/step - loss: 0.1701 - val_loss: 0.1632\n",
      "Epoch 178/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1674\n",
      "Epoch 178: val_loss improved from 0.16107 to 0.15851, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1674 - val_loss: 0.1585\n",
      "Epoch 179/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1679\n",
      "Epoch 179: val_loss improved from 0.15851 to 0.15693, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1679 - val_loss: 0.1569\n",
      "Epoch 180/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1687\n",
      "Epoch 180: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1687 - val_loss: 0.1665\n",
      "Epoch 181/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1687\n",
      "Epoch 181: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1687 - val_loss: 0.1617\n",
      "Epoch 182/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1679\n",
      "Epoch 182: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1679 - val_loss: 0.1626\n",
      "Epoch 183/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1689\n",
      "Epoch 183: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1689 - val_loss: 0.1588\n",
      "Epoch 184/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1690\n",
      "Epoch 184: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1690 - val_loss: 0.1588\n",
      "Epoch 185/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1674\n",
      "Epoch 185: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1674 - val_loss: 0.1598\n",
      "Epoch 186/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1670\n",
      "Epoch 186: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1670 - val_loss: 0.1571\n",
      "Epoch 187/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1684\n",
      "Epoch 187: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1684 - val_loss: 0.1600\n",
      "Epoch 188/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1672\n",
      "Epoch 188: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1672 - val_loss: 0.1603\n",
      "Epoch 189/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1665\n",
      "Epoch 189: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 199ms/step - loss: 0.1665 - val_loss: 0.1571\n",
      "Epoch 190/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1671\n",
      "Epoch 190: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1671 - val_loss: 0.1629\n",
      "Epoch 191/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1655\n",
      "Epoch 191: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 200ms/step - loss: 0.1655 - val_loss: 0.1579\n",
      "Epoch 192/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1662\n",
      "Epoch 192: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1662 - val_loss: 0.1658\n",
      "Epoch 193/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1642\n",
      "Epoch 193: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1642 - val_loss: 0.1572\n",
      "Epoch 194/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1652\n",
      "Epoch 194: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 20s 217ms/step - loss: 0.1652 - val_loss: 0.1613\n",
      "Epoch 195/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1642\n",
      "Epoch 195: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 21s 219ms/step - loss: 0.1642 - val_loss: 0.1634\n",
      "Epoch 196/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1646\n",
      "Epoch 196: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 20s 212ms/step - loss: 0.1646 - val_loss: 0.1610\n",
      "Epoch 197/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1659\n",
      "Epoch 197: val_loss did not improve from 0.15693\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1659 - val_loss: 0.1622\n",
      "Epoch 198/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1648\n",
      "Epoch 198: val_loss improved from 0.15693 to 0.15140, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1648 - val_loss: 0.1514\n",
      "Epoch 199/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1634\n",
      "Epoch 199: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1634 - val_loss: 0.1546\n",
      "Epoch 200/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1647\n",
      "Epoch 200: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1647 - val_loss: 0.1536\n",
      "Epoch 201/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1659\n",
      "Epoch 201: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1659 - val_loss: 0.1567\n",
      "Epoch 202/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1614\n",
      "Epoch 202: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1614 - val_loss: 0.1547\n",
      "Epoch 203/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1613\n",
      "Epoch 203: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 201ms/step - loss: 0.1613 - val_loss: 0.1627\n",
      "Epoch 204/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1621\n",
      "Epoch 204: val_loss did not improve from 0.15140\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1621 - val_loss: 0.1619\n",
      "Epoch 205/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1643\n",
      "Epoch 205: val_loss improved from 0.15140 to 0.15021, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 208ms/step - loss: 0.1643 - val_loss: 0.1502\n",
      "Epoch 206/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1637\n",
      "Epoch 206: val_loss did not improve from 0.15021\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1637 - val_loss: 0.1573\n",
      "Epoch 207/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1608\n",
      "Epoch 207: val_loss did not improve from 0.15021\n",
      "94/94 [==============================] - 19s 202ms/step - loss: 0.1608 - val_loss: 0.1558\n",
      "Epoch 208/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1622\n",
      "Epoch 208: val_loss improved from 0.15021 to 0.14896, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1622 - val_loss: 0.1490\n",
      "Epoch 209/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1609\n",
      "Epoch 209: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1609 - val_loss: 0.1525\n",
      "Epoch 210/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1595\n",
      "Epoch 210: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1595 - val_loss: 0.1530\n",
      "Epoch 211/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1592\n",
      "Epoch 211: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1592 - val_loss: 0.1602\n",
      "Epoch 212/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1596\n",
      "Epoch 212: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1596 - val_loss: 0.1516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1635\n",
      "Epoch 213: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 205ms/step - loss: 0.1635 - val_loss: 0.1516\n",
      "Epoch 214/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1595\n",
      "Epoch 214: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1595 - val_loss: 0.1534\n",
      "Epoch 215/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1599\n",
      "Epoch 215: val_loss did not improve from 0.14896\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1599 - val_loss: 0.1578\n",
      "Epoch 216/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1584\n",
      "Epoch 216: val_loss improved from 0.14896 to 0.14881, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 209ms/step - loss: 0.1584 - val_loss: 0.1488\n",
      "Epoch 217/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1587\n",
      "Epoch 217: val_loss did not improve from 0.14881\n",
      "94/94 [==============================] - 20s 207ms/step - loss: 0.1587 - val_loss: 0.1496\n",
      "Epoch 218/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1593\n",
      "Epoch 218: val_loss did not improve from 0.14881\n",
      "94/94 [==============================] - 20s 213ms/step - loss: 0.1593 - val_loss: 0.1511\n",
      "Epoch 219/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1595\n",
      "Epoch 219: val_loss did not improve from 0.14881\n",
      "94/94 [==============================] - 19s 204ms/step - loss: 0.1595 - val_loss: 0.1527\n",
      "Epoch 220/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1575\n",
      "Epoch 220: val_loss did not improve from 0.14881\n",
      "94/94 [==============================] - 19s 203ms/step - loss: 0.1575 - val_loss: 0.1547\n",
      "Epoch 221/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1588\n",
      "Epoch 221: val_loss improved from 0.14881 to 0.14819, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1588 - val_loss: 0.1482\n",
      "Epoch 222/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1588\n",
      "Epoch 222: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 19s 206ms/step - loss: 0.1588 - val_loss: 0.1553\n",
      "Epoch 223/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1580\n",
      "Epoch 223: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 20s 208ms/step - loss: 0.1580 - val_loss: 0.1541\n",
      "Epoch 224/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1588\n",
      "Epoch 224: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 19s 207ms/step - loss: 0.1588 - val_loss: 0.1495\n",
      "Epoch 225/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1564\n",
      "Epoch 225: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 64s 691ms/step - loss: 0.1564 - val_loss: 0.1551\n",
      "Epoch 226/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1573\n",
      "Epoch 226: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 76s 814ms/step - loss: 0.1573 - val_loss: 0.1529\n",
      "Epoch 227/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1603\n",
      "Epoch 227: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 81s 868ms/step - loss: 0.1603 - val_loss: 0.1515\n",
      "Epoch 228/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1553\n",
      "Epoch 228: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 81s 867ms/step - loss: 0.1553 - val_loss: 0.1565\n",
      "Epoch 229/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1562\n",
      "Epoch 229: val_loss did not improve from 0.14819\n",
      "94/94 [==============================] - 76s 805ms/step - loss: 0.1562 - val_loss: 0.1484\n",
      "Epoch 230/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1583\n",
      "Epoch 230: val_loss improved from 0.14819 to 0.14604, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 83s 880ms/step - loss: 0.1583 - val_loss: 0.1460\n",
      "Epoch 231/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1553\n",
      "Epoch 231: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 84s 897ms/step - loss: 0.1553 - val_loss: 0.1549\n",
      "Epoch 232/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1581\n",
      "Epoch 232: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 75s 796ms/step - loss: 0.1581 - val_loss: 0.1521\n",
      "Epoch 233/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1563\n",
      "Epoch 233: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 50s 535ms/step - loss: 0.1563 - val_loss: 0.1475\n",
      "Epoch 234/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1564\n",
      "Epoch 234: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 57s 598ms/step - loss: 0.1564 - val_loss: 0.1519\n",
      "Epoch 235/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1549\n",
      "Epoch 235: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 62s 659ms/step - loss: 0.1549 - val_loss: 0.1473\n",
      "Epoch 236/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1554\n",
      "Epoch 236: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 74s 783ms/step - loss: 0.1554 - val_loss: 0.1583\n",
      "Epoch 237/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1576\n",
      "Epoch 237: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 78s 834ms/step - loss: 0.1576 - val_loss: 0.1474\n",
      "Epoch 238/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1546\n",
      "Epoch 238: val_loss did not improve from 0.14604\n",
      "94/94 [==============================] - 81s 859ms/step - loss: 0.1546 - val_loss: 0.1505\n",
      "Epoch 239/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1546\n",
      "Epoch 239: val_loss improved from 0.14604 to 0.14358, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 88s 935ms/step - loss: 0.1546 - val_loss: 0.1436\n",
      "Epoch 240/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1538\n",
      "Epoch 240: val_loss did not improve from 0.14358\n",
      "94/94 [==============================] - 79s 841ms/step - loss: 0.1538 - val_loss: 0.1475\n",
      "Epoch 241/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1531\n",
      "Epoch 241: val_loss did not improve from 0.14358\n",
      "94/94 [==============================] - 82s 870ms/step - loss: 0.1531 - val_loss: 0.1501\n",
      "Epoch 242/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1542\n",
      "Epoch 242: val_loss did not improve from 0.14358\n",
      "94/94 [==============================] - 66s 702ms/step - loss: 0.1542 - val_loss: 0.1521\n",
      "Epoch 243/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1566\n",
      "Epoch 243: val_loss improved from 0.14358 to 0.14224, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 74s 790ms/step - loss: 0.1566 - val_loss: 0.1422\n",
      "Epoch 244/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1548\n",
      "Epoch 244: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 47s 501ms/step - loss: 0.1548 - val_loss: 0.1448\n",
      "Epoch 245/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1520\n",
      "Epoch 245: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 82s 868ms/step - loss: 0.1520 - val_loss: 0.1445\n",
      "Epoch 246/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 246: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 60s 643ms/step - loss: 0.1523 - val_loss: 0.1430\n",
      "Epoch 247/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1535\n",
      "Epoch 247: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 78s 825ms/step - loss: 0.1535 - val_loss: 0.1481\n",
      "Epoch 248/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1512\n",
      "Epoch 248: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 70s 741ms/step - loss: 0.1512 - val_loss: 0.1483\n",
      "Epoch 249/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 249: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 42s 442ms/step - loss: 0.1523 - val_loss: 0.1435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1523\n",
      "Epoch 250: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 69s 738ms/step - loss: 0.1523 - val_loss: 0.1483\n",
      "Epoch 251/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1526\n",
      "Epoch 251: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 71s 764ms/step - loss: 0.1526 - val_loss: 0.1473\n",
      "Epoch 252/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1525\n",
      "Epoch 252: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 85s 908ms/step - loss: 0.1525 - val_loss: 0.1481\n",
      "Epoch 253/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1520\n",
      "Epoch 253: val_loss did not improve from 0.14224\n",
      "94/94 [==============================] - 84s 895ms/step - loss: 0.1520 - val_loss: 0.1439\n",
      "Epoch 254/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1510\n",
      "Epoch 254: val_loss improved from 0.14224 to 0.14202, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 63s 672ms/step - loss: 0.1510 - val_loss: 0.1420\n",
      "Epoch 255/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1508\n",
      "Epoch 255: val_loss did not improve from 0.14202\n",
      "94/94 [==============================] - 81s 862ms/step - loss: 0.1508 - val_loss: 0.1430\n",
      "Epoch 256/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1505\n",
      "Epoch 256: val_loss did not improve from 0.14202\n",
      "94/94 [==============================] - 74s 791ms/step - loss: 0.1505 - val_loss: 0.1462\n",
      "Epoch 257/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1518\n",
      "Epoch 257: val_loss improved from 0.14202 to 0.13923, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 73s 778ms/step - loss: 0.1518 - val_loss: 0.1392\n",
      "Epoch 258/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1506\n",
      "Epoch 258: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 80s 846ms/step - loss: 0.1506 - val_loss: 0.1453\n",
      "Epoch 259/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1485\n",
      "Epoch 259: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 77s 815ms/step - loss: 0.1485 - val_loss: 0.1426\n",
      "Epoch 260/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1519\n",
      "Epoch 260: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 80s 853ms/step - loss: 0.1519 - val_loss: 0.1436\n",
      "Epoch 261/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1495\n",
      "Epoch 261: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 76s 813ms/step - loss: 0.1495 - val_loss: 0.1421\n",
      "Epoch 262/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1488\n",
      "Epoch 262: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 66s 704ms/step - loss: 0.1488 - val_loss: 0.1430\n",
      "Epoch 263/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1497\n",
      "Epoch 263: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 55s 594ms/step - loss: 0.1497 - val_loss: 0.1397\n",
      "Epoch 264/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1493\n",
      "Epoch 264: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 49s 522ms/step - loss: 0.1493 - val_loss: 0.1442\n",
      "Epoch 265/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1511\n",
      "Epoch 265: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 35s 363ms/step - loss: 0.1511 - val_loss: 0.1394\n",
      "Epoch 266/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1478\n",
      "Epoch 266: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 31s 332ms/step - loss: 0.1478 - val_loss: 0.1456\n",
      "Epoch 267/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1507\n",
      "Epoch 267: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 32s 344ms/step - loss: 0.1507 - val_loss: 0.1438\n",
      "Epoch 268/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1481\n",
      "Epoch 268: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 37s 391ms/step - loss: 0.1481 - val_loss: 0.1411\n",
      "Epoch 269/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1478\n",
      "Epoch 269: val_loss did not improve from 0.13923\n",
      "94/94 [==============================] - 50s 536ms/step - loss: 0.1478 - val_loss: 0.1441\n",
      "Epoch 270/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1481\n",
      "Epoch 270: val_loss improved from 0.13923 to 0.13812, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 69s 740ms/step - loss: 0.1481 - val_loss: 0.1381\n",
      "Epoch 271/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1472\n",
      "Epoch 271: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 72s 767ms/step - loss: 0.1472 - val_loss: 0.1406\n",
      "Epoch 272/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1477\n",
      "Epoch 272: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 78s 829ms/step - loss: 0.1477 - val_loss: 0.1434\n",
      "Epoch 273/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1482\n",
      "Epoch 273: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 79s 837ms/step - loss: 0.1482 - val_loss: 0.1431\n",
      "Epoch 274/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1486\n",
      "Epoch 274: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 76s 806ms/step - loss: 0.1486 - val_loss: 0.1402\n",
      "Epoch 275/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1491\n",
      "Epoch 275: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 79s 845ms/step - loss: 0.1491 - val_loss: 0.1454\n",
      "Epoch 276/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1478\n",
      "Epoch 276: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 78s 835ms/step - loss: 0.1478 - val_loss: 0.1466\n",
      "Epoch 277/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1484\n",
      "Epoch 277: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 76s 805ms/step - loss: 0.1484 - val_loss: 0.1458\n",
      "Epoch 278/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1473\n",
      "Epoch 278: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 62s 667ms/step - loss: 0.1473 - val_loss: 0.1407\n",
      "Epoch 279/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1451\n",
      "Epoch 279: val_loss did not improve from 0.13812\n",
      "94/94 [==============================] - 54s 580ms/step - loss: 0.1451 - val_loss: 0.1383\n",
      "Epoch 280/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1465\n",
      "Epoch 280: val_loss improved from 0.13812 to 0.13541, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 28s 294ms/step - loss: 0.1465 - val_loss: 0.1354\n",
      "Epoch 281/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1473\n",
      "Epoch 281: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 59s 620ms/step - loss: 0.1473 - val_loss: 0.1387\n",
      "Epoch 282/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1465\n",
      "Epoch 282: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 69s 734ms/step - loss: 0.1465 - val_loss: 0.1378\n",
      "Epoch 283/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1447\n",
      "Epoch 283: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 72s 764ms/step - loss: 0.1447 - val_loss: 0.1435\n",
      "Epoch 284/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1459\n",
      "Epoch 284: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 75s 799ms/step - loss: 0.1459 - val_loss: 0.1422\n",
      "Epoch 285/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1453\n",
      "Epoch 285: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 74s 789ms/step - loss: 0.1453 - val_loss: 0.1431\n",
      "Epoch 286/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1454\n",
      "Epoch 286: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 61s 651ms/step - loss: 0.1454 - val_loss: 0.1395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1452\n",
      "Epoch 287: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 83s 883ms/step - loss: 0.1452 - val_loss: 0.1373\n",
      "Epoch 288/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1452\n",
      "Epoch 288: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 82s 878ms/step - loss: 0.1452 - val_loss: 0.1477\n",
      "Epoch 289/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1460\n",
      "Epoch 289: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 81s 866ms/step - loss: 0.1460 - val_loss: 0.1366\n",
      "Epoch 290/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1447\n",
      "Epoch 290: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 67s 711ms/step - loss: 0.1447 - val_loss: 0.1432\n",
      "Epoch 291/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1511\n",
      "Epoch 291: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 76s 804ms/step - loss: 0.1511 - val_loss: 0.1421\n",
      "Epoch 292/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1429\n",
      "Epoch 292: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 68s 722ms/step - loss: 0.1429 - val_loss: 0.1371\n",
      "Epoch 293/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1464\n",
      "Epoch 293: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 44s 461ms/step - loss: 0.1464 - val_loss: 0.1379\n",
      "Epoch 294/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1439\n",
      "Epoch 294: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 57s 614ms/step - loss: 0.1439 - val_loss: 0.1386\n",
      "Epoch 295/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1449\n",
      "Epoch 295: val_loss did not improve from 0.13541\n",
      "94/94 [==============================] - 28s 293ms/step - loss: 0.1449 - val_loss: 0.1364\n",
      "Epoch 296/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 296: val_loss improved from 0.13541 to 0.13438, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 33s 357ms/step - loss: 0.1423 - val_loss: 0.1344\n",
      "Epoch 297/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1444\n",
      "Epoch 297: val_loss did not improve from 0.13438\n",
      "94/94 [==============================] - 43s 456ms/step - loss: 0.1444 - val_loss: 0.1378\n",
      "Epoch 298/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1422\n",
      "Epoch 298: val_loss improved from 0.13438 to 0.13436, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 49s 524ms/step - loss: 0.1422 - val_loss: 0.1344\n",
      "Epoch 299/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1422\n",
      "Epoch 299: val_loss improved from 0.13436 to 0.13433, saving model to transformer_model_hmf.h5\n",
      "94/94 [==============================] - 43s 449ms/step - loss: 0.1422 - val_loss: 0.1343\n",
      "Epoch 300/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 300: val_loss did not improve from 0.13433\n",
      "94/94 [==============================] - 50s 525ms/step - loss: 0.1423 - val_loss: 0.1357\n"
     ]
    }
   ],
   "source": [
    "# def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "#     # Attention and Normalization\n",
    "#     x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "#     x = layers.Dropout(dropout)(x)\n",
    "#     x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "#     # Feed Forward Part\n",
    "#     ffn = tf.keras.Sequential([\n",
    "#         layers.Dense(ff_dim, activation=\"relu\"),\n",
    "#         layers.Dropout(dropout),\n",
    "#         layers.Dense(inputs.shape[-1]),\n",
    "#     ])\n",
    "#     x = ffn(x)\n",
    "#     x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "#     return x\n",
    "\n",
    "# def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = inputs\n",
    "\n",
    "#     for _ in range(num_layers):\n",
    "#         x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "#     # Selecting the last timestep output\n",
    "#     x = x[:, -1, :]  # This slices out the last timestep\n",
    "\n",
    "#     outputs = layers.Dense(molecules*3, activation='linear')(x)\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# # Model parameters\n",
    "# head_size = 256\n",
    "# num_heads = 4\n",
    "# ff_dim = 256\n",
    "# num_layers = 4\n",
    "# dropout_rate = 0.1\n",
    "\n",
    "# # Build the model\n",
    "# model = build_transformer_model(input_shape=(seq_length, molecules*3), head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, num_layers=num_layers, dropout=dropout_rate)\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001), loss='mape')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=50, verbose=1, mode=\"min\")\n",
    "# checkpoint = ModelCheckpoint(filepath='transformer_model_%s.h5' % species, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# history = model.fit(x1_train, y1_train, callbacks=[callback, checkpoint], batch_size=batch_size, epochs=300, verbose=1, validation_data=(x1_val, y1_val), shuffle=False)\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "# hist_df.to_csv('11history_%s_4.csv' %species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15882aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the previously saved model\n",
    "# model_path = 'transformer_model_hmf.h5'\n",
    "# model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb04f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Re-compile the model with potentially adjusted learning rate or optimizer\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001), loss='mape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc25abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1382\n",
      "Epoch 1: val_loss improved from inf to 0.12950, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 52s 97ms/step - loss: 0.1382 - val_loss: 0.1295\n",
      "Epoch 2/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1380\n",
      "Epoch 2: val_loss did not improve from 0.12950\n",
      "375/375 [==============================] - 61s 163ms/step - loss: 0.1380 - val_loss: 0.1297\n",
      "Epoch 3/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1375\n",
      "Epoch 3: val_loss improved from 0.12950 to 0.12918, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 80s 213ms/step - loss: 0.1375 - val_loss: 0.1292\n",
      "Epoch 4/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1379\n",
      "Epoch 4: val_loss did not improve from 0.12918\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 0.1379 - val_loss: 0.1299\n",
      "Epoch 5/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1374\n",
      "Epoch 5: val_loss did not improve from 0.12918\n",
      "375/375 [==============================] - 74s 199ms/step - loss: 0.1374 - val_loss: 0.1320\n",
      "Epoch 6/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1373\n",
      "Epoch 6: val_loss improved from 0.12918 to 0.12911, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1373 - val_loss: 0.1291\n",
      "Epoch 7/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1373\n",
      "Epoch 7: val_loss did not improve from 0.12911\n",
      "375/375 [==============================] - 58s 156ms/step - loss: 0.1373 - val_loss: 0.1302\n",
      "Epoch 8/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1369\n",
      "Epoch 8: val_loss did not improve from 0.12911\n",
      "375/375 [==============================] - 77s 206ms/step - loss: 0.1369 - val_loss: 0.1294\n",
      "Epoch 9/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1366\n",
      "Epoch 9: val_loss improved from 0.12911 to 0.12905, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 0.1366 - val_loss: 0.1290\n",
      "Epoch 10/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1371\n",
      "Epoch 10: val_loss did not improve from 0.12905\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 0.1371 - val_loss: 0.1291\n",
      "Epoch 11/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1370\n",
      "Epoch 11: val_loss improved from 0.12905 to 0.12826, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 76s 201ms/step - loss: 0.1370 - val_loss: 0.1283\n",
      "Epoch 12/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1368\n",
      "Epoch 12: val_loss improved from 0.12826 to 0.12771, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 76s 203ms/step - loss: 0.1368 - val_loss: 0.1277\n",
      "Epoch 13/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1367\n",
      "Epoch 13: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 55s 146ms/step - loss: 0.1367 - val_loss: 0.1304\n",
      "Epoch 14/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1363\n",
      "Epoch 14: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 72s 193ms/step - loss: 0.1363 - val_loss: 0.1283\n",
      "Epoch 15/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1361\n",
      "Epoch 15: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 50s 132ms/step - loss: 0.1361 - val_loss: 0.1286\n",
      "Epoch 16/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1369\n",
      "Epoch 16: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 64s 170ms/step - loss: 0.1369 - val_loss: 0.1279\n",
      "Epoch 17/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1361\n",
      "Epoch 17: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 79s 211ms/step - loss: 0.1361 - val_loss: 0.1283\n",
      "Epoch 18/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1365\n",
      "Epoch 18: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 0.1365 - val_loss: 0.1290\n",
      "Epoch 19/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1363\n",
      "Epoch 19: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 0.1363 - val_loss: 0.1284\n",
      "Epoch 20/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 20: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 80s 212ms/step - loss: 0.1359 - val_loss: 0.1287\n",
      "Epoch 21/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1361\n",
      "Epoch 21: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 57s 151ms/step - loss: 0.1361 - val_loss: 0.1287\n",
      "Epoch 22/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1357\n",
      "Epoch 22: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 0.1357 - val_loss: 0.1280\n",
      "Epoch 23/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 23: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 76s 203ms/step - loss: 0.1359 - val_loss: 0.1280\n",
      "Epoch 24/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1360\n",
      "Epoch 24: val_loss did not improve from 0.12771\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 0.1360 - val_loss: 0.1280\n",
      "Epoch 25/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 25: val_loss improved from 0.12771 to 0.12753, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 66s 177ms/step - loss: 0.1354 - val_loss: 0.1275\n",
      "Epoch 26/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1355\n",
      "Epoch 26: val_loss improved from 0.12753 to 0.12690, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 79s 211ms/step - loss: 0.1355 - val_loss: 0.1269\n",
      "Epoch 27/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1358\n",
      "Epoch 27: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 0.1358 - val_loss: 0.1274\n",
      "Epoch 28/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 28: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 77s 205ms/step - loss: 0.1354 - val_loss: 0.1272\n",
      "Epoch 29/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1354\n",
      "Epoch 29: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1354 - val_loss: 0.1273\n",
      "Epoch 30/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 30: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 82s 218ms/step - loss: 0.1352 - val_loss: 0.1282\n",
      "Epoch 31/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1356\n",
      "Epoch 31: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 77s 206ms/step - loss: 0.1356 - val_loss: 0.1272\n",
      "Epoch 32/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 32: val_loss did not improve from 0.12690\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 0.1352 - val_loss: 0.1284\n",
      "Epoch 33/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1347\n",
      "Epoch 33: val_loss improved from 0.12690 to 0.12672, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 54s 144ms/step - loss: 0.1347 - val_loss: 0.1267\n",
      "Epoch 34/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1353\n",
      "Epoch 34: val_loss improved from 0.12672 to 0.12577, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 53s 143ms/step - loss: 0.1353 - val_loss: 0.1258\n",
      "Epoch 35/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1351\n",
      "Epoch 35: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 63s 167ms/step - loss: 0.1351 - val_loss: 0.1268\n",
      "Epoch 36/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1348\n",
      "Epoch 36: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 0.1348 - val_loss: 0.1264\n",
      "Epoch 37/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1347\n",
      "Epoch 37: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 64s 169ms/step - loss: 0.1347 - val_loss: 0.1281\n",
      "Epoch 38/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1347\n",
      "Epoch 38: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 78s 207ms/step - loss: 0.1347 - val_loss: 0.1297\n",
      "Epoch 39/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1346\n",
      "Epoch 39: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 64s 171ms/step - loss: 0.1346 - val_loss: 0.1283\n",
      "Epoch 40/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1345\n",
      "Epoch 40: val_loss did not improve from 0.12577\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 0.1345 - val_loss: 0.1281\n",
      "Epoch 41/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 41: val_loss improved from 0.12577 to 0.12559, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1344 - val_loss: 0.1256\n",
      "Epoch 42/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1344\n",
      "Epoch 42: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1344 - val_loss: 0.1274\n",
      "Epoch 43/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1341\n",
      "Epoch 43: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1341 - val_loss: 0.1265\n",
      "Epoch 44/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1343\n",
      "Epoch 44: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1343 - val_loss: 0.1274\n",
      "Epoch 45/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1340\n",
      "Epoch 45: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1340 - val_loss: 0.1266\n",
      "Epoch 46/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1339\n",
      "Epoch 46: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1339 - val_loss: 0.1269\n",
      "Epoch 47/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1338\n",
      "Epoch 47: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 0.1338 - val_loss: 0.1290\n",
      "Epoch 48/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1340\n",
      "Epoch 48: val_loss did not improve from 0.12559\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 0.1340 - val_loss: 0.1272\n",
      "Epoch 49/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1334\n",
      "Epoch 49: val_loss improved from 0.12559 to 0.12521, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1334 - val_loss: 0.1252\n",
      "Epoch 50/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1335\n",
      "Epoch 50: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1335 - val_loss: 0.1260\n",
      "Epoch 51/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1335\n",
      "Epoch 51: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1335 - val_loss: 0.1266\n",
      "Epoch 52/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1334\n",
      "Epoch 52: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.1334 - val_loss: 0.1267\n",
      "Epoch 53/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1331\n",
      "Epoch 53: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1331 - val_loss: 0.1260\n",
      "Epoch 54/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1328\n",
      "Epoch 54: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 0.1328 - val_loss: 0.1253\n",
      "Epoch 55/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1330\n",
      "Epoch 55: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.1330 - val_loss: 0.1259\n",
      "Epoch 56/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1331\n",
      "Epoch 56: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 0.1331 - val_loss: 0.1272\n",
      "Epoch 57/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1332\n",
      "Epoch 57: val_loss did not improve from 0.12521\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 0.1332 - val_loss: 0.1255\n",
      "Epoch 58/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1332\n",
      "Epoch 58: val_loss improved from 0.12521 to 0.12438, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1332 - val_loss: 0.1244\n",
      "Epoch 59/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1331\n",
      "Epoch 59: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1331 - val_loss: 0.1271\n",
      "Epoch 60/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1326\n",
      "Epoch 60: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1326 - val_loss: 0.1249\n",
      "Epoch 61/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1325\n",
      "Epoch 61: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1325 - val_loss: 0.1251\n",
      "Epoch 62/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1322\n",
      "Epoch 62: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1322 - val_loss: 0.1252\n",
      "Epoch 63/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1325\n",
      "Epoch 63: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 0.1325 - val_loss: 0.1244\n",
      "Epoch 64/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1328\n",
      "Epoch 64: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 0.1328 - val_loss: 0.1252\n",
      "Epoch 65/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1327\n",
      "Epoch 65: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 0.1327 - val_loss: 0.1255\n",
      "Epoch 66/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1325\n",
      "Epoch 66: val_loss did not improve from 0.12438\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1325 - val_loss: 0.1247\n",
      "Epoch 67/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 67: val_loss improved from 0.12438 to 0.12408, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1321 - val_loss: 0.1241\n",
      "Epoch 68/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 68: val_loss improved from 0.12408 to 0.12387, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1321 - val_loss: 0.1239\n",
      "Epoch 69/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1322\n",
      "Epoch 69: val_loss did not improve from 0.12387\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1322 - val_loss: 0.1265\n",
      "Epoch 70/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 70: val_loss improved from 0.12387 to 0.12362, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1321 - val_loss: 0.1236\n",
      "Epoch 71/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 71: val_loss did not improve from 0.12362\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1318 - val_loss: 0.1245\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 72: val_loss improved from 0.12362 to 0.12323, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1318 - val_loss: 0.1232\n",
      "Epoch 73/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 73: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1316 - val_loss: 0.1244\n",
      "Epoch 74/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 74: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 0.1321 - val_loss: 0.1253\n",
      "Epoch 75/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 75: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 0.1318 - val_loss: 0.1260\n",
      "Epoch 76/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 76: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1319 - val_loss: 0.1240\n",
      "Epoch 77/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 77: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.1316 - val_loss: 0.1259\n",
      "Epoch 78/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1314\n",
      "Epoch 78: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1314 - val_loss: 0.1246\n",
      "Epoch 79/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1313\n",
      "Epoch 79: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 0.1313 - val_loss: 0.1242\n",
      "Epoch 80/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1314\n",
      "Epoch 80: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1314 - val_loss: 0.1237\n",
      "Epoch 81/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1312\n",
      "Epoch 81: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1312 - val_loss: 0.1239\n",
      "Epoch 82/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1312\n",
      "Epoch 82: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1312 - val_loss: 0.1232\n",
      "Epoch 83/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1306\n",
      "Epoch 83: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1306 - val_loss: 0.1234\n",
      "Epoch 84/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1310\n",
      "Epoch 84: val_loss did not improve from 0.12323\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1310 - val_loss: 0.1239\n",
      "Epoch 85/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1308\n",
      "Epoch 85: val_loss improved from 0.12323 to 0.12294, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 0.1308 - val_loss: 0.1229\n",
      "Epoch 86/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1309\n",
      "Epoch 86: val_loss improved from 0.12294 to 0.12249, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 183ms/step - loss: 0.1309 - val_loss: 0.1225\n",
      "Epoch 87/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1310\n",
      "Epoch 87: val_loss did not improve from 0.12249\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1310 - val_loss: 0.1230\n",
      "Epoch 88/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1304\n",
      "Epoch 88: val_loss did not improve from 0.12249\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1304 - val_loss: 0.1237\n",
      "Epoch 89/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1305\n",
      "Epoch 89: val_loss did not improve from 0.12249\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 0.1305 - val_loss: 0.1243\n",
      "Epoch 90/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1305\n",
      "Epoch 90: val_loss improved from 0.12249 to 0.12204, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 0.1305 - val_loss: 0.1220\n",
      "Epoch 91/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 91: val_loss did not improve from 0.12204\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 0.1303 - val_loss: 0.1232\n",
      "Epoch 92/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 92: val_loss did not improve from 0.12204\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1303 - val_loss: 0.1228\n",
      "Epoch 93/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1300\n",
      "Epoch 93: val_loss did not improve from 0.12204\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1300 - val_loss: 0.1225\n",
      "Epoch 94/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1302\n",
      "Epoch 94: val_loss improved from 0.12204 to 0.12181, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1302 - val_loss: 0.1218\n",
      "Epoch 95/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 95: val_loss did not improve from 0.12181\n",
      "375/375 [==============================] - 66s 177ms/step - loss: 0.1299 - val_loss: 0.1248\n",
      "Epoch 96/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 96: val_loss did not improve from 0.12181\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1301 - val_loss: 0.1219\n",
      "Epoch 97/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1302\n",
      "Epoch 97: val_loss did not improve from 0.12181\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 0.1302 - val_loss: 0.1241\n",
      "Epoch 98/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 98: val_loss did not improve from 0.12181\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1299 - val_loss: 0.1231\n",
      "Epoch 99/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1298\n",
      "Epoch 99: val_loss improved from 0.12181 to 0.12171, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.1298 - val_loss: 0.1217\n",
      "Epoch 100/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1296\n",
      "Epoch 100: val_loss did not improve from 0.12171\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.1296 - val_loss: 0.1234\n",
      "Epoch 101/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1296\n",
      "Epoch 101: val_loss improved from 0.12171 to 0.12134, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 183ms/step - loss: 0.1296 - val_loss: 0.1213\n",
      "Epoch 102/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1294\n",
      "Epoch 102: val_loss did not improve from 0.12134\n",
      "375/375 [==============================] - 67s 180ms/step - loss: 0.1294 - val_loss: 0.1234\n",
      "Epoch 103/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1294\n",
      "Epoch 103: val_loss did not improve from 0.12134\n",
      "375/375 [==============================] - 64s 170ms/step - loss: 0.1294 - val_loss: 0.1213\n",
      "Epoch 104/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1293\n",
      "Epoch 104: val_loss improved from 0.12134 to 0.12126, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 47s 126ms/step - loss: 0.1293 - val_loss: 0.1213\n",
      "Epoch 105/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1294\n",
      "Epoch 105: val_loss did not improve from 0.12126\n",
      "375/375 [==============================] - 63s 168ms/step - loss: 0.1294 - val_loss: 0.1222\n",
      "Epoch 106/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 106: val_loss did not improve from 0.12126\n",
      "375/375 [==============================] - 97s 259ms/step - loss: 0.1288 - val_loss: 0.1224\n",
      "Epoch 107/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1294\n",
      "Epoch 107: val_loss improved from 0.12126 to 0.12064, saving model to transformer_model_updated.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1294 - val_loss: 0.1206\n",
      "Epoch 108/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 108: val_loss did not improve from 0.12064\n",
      "375/375 [==============================] - 92s 246ms/step - loss: 0.1291 - val_loss: 0.1214\n",
      "Epoch 109/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 109: val_loss improved from 0.12064 to 0.12034, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 90s 239ms/step - loss: 0.1288 - val_loss: 0.1203\n",
      "Epoch 110/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1287\n",
      "Epoch 110: val_loss did not improve from 0.12034\n",
      "375/375 [==============================] - 75s 199ms/step - loss: 0.1287 - val_loss: 0.1210\n",
      "Epoch 111/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 111: val_loss did not improve from 0.12034\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.1286 - val_loss: 0.1209\n",
      "Epoch 112/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1287\n",
      "Epoch 112: val_loss did not improve from 0.12034\n",
      "375/375 [==============================] - 92s 245ms/step - loss: 0.1287 - val_loss: 0.1211\n",
      "Epoch 113/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 113: val_loss did not improve from 0.12034\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1286 - val_loss: 0.1205\n",
      "Epoch 114/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 114: val_loss improved from 0.12034 to 0.11981, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 90s 241ms/step - loss: 0.1286 - val_loss: 0.1198\n",
      "Epoch 115/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 115: val_loss did not improve from 0.11981\n",
      "375/375 [==============================] - 93s 248ms/step - loss: 0.1281 - val_loss: 0.1200\n",
      "Epoch 116/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 116: val_loss did not improve from 0.11981\n",
      "375/375 [==============================] - 92s 244ms/step - loss: 0.1281 - val_loss: 0.1208\n",
      "Epoch 117/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 117: val_loss did not improve from 0.11981\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1279 - val_loss: 0.1224\n",
      "Epoch 118/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 118: val_loss did not improve from 0.11981\n",
      "375/375 [==============================] - 93s 249ms/step - loss: 0.1281 - val_loss: 0.1211\n",
      "Epoch 119/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1274\n",
      "Epoch 119: val_loss did not improve from 0.11981\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.1274 - val_loss: 0.1218\n",
      "Epoch 120/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 120: val_loss improved from 0.11981 to 0.11958, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 77s 206ms/step - loss: 0.1279 - val_loss: 0.1196\n",
      "Epoch 121/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1276\n",
      "Epoch 121: val_loss did not improve from 0.11958\n",
      "375/375 [==============================] - 77s 207ms/step - loss: 0.1276 - val_loss: 0.1213\n",
      "Epoch 122/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1278\n",
      "Epoch 122: val_loss improved from 0.11958 to 0.11945, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 0.1278 - val_loss: 0.1195\n",
      "Epoch 123/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1277\n",
      "Epoch 123: val_loss did not improve from 0.11945\n",
      "375/375 [==============================] - 66s 175ms/step - loss: 0.1277 - val_loss: 0.1214\n",
      "Epoch 124/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1275\n",
      "Epoch 124: val_loss improved from 0.11945 to 0.11854, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 0.1275 - val_loss: 0.1185\n",
      "Epoch 125/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 125: val_loss did not improve from 0.11854\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 0.1272 - val_loss: 0.1204\n",
      "Epoch 126/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1270\n",
      "Epoch 126: val_loss did not improve from 0.11854\n",
      "375/375 [==============================] - 81s 216ms/step - loss: 0.1270 - val_loss: 0.1198\n",
      "Epoch 127/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 127: val_loss did not improve from 0.11854\n",
      "375/375 [==============================] - 85s 225ms/step - loss: 0.1272 - val_loss: 0.1193\n",
      "Epoch 128/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 128: val_loss did not improve from 0.11854\n",
      "375/375 [==============================] - 88s 234ms/step - loss: 0.1268 - val_loss: 0.1190\n",
      "Epoch 129/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1271\n",
      "Epoch 129: val_loss did not improve from 0.11854\n",
      "375/375 [==============================] - 84s 223ms/step - loss: 0.1271 - val_loss: 0.1211\n",
      "Epoch 130/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1267\n",
      "Epoch 130: val_loss improved from 0.11854 to 0.11839, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.1267 - val_loss: 0.1184\n",
      "Epoch 131/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 131: val_loss did not improve from 0.11839\n",
      "375/375 [==============================] - 87s 231ms/step - loss: 0.1268 - val_loss: 0.1202\n",
      "Epoch 132/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1266\n",
      "Epoch 132: val_loss did not improve from 0.11839\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1266 - val_loss: 0.1190\n",
      "Epoch 133/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1264\n",
      "Epoch 133: val_loss did not improve from 0.11839\n",
      "375/375 [==============================] - 86s 228ms/step - loss: 0.1264 - val_loss: 0.1184\n",
      "Epoch 134/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1264\n",
      "Epoch 134: val_loss improved from 0.11839 to 0.11784, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1264 - val_loss: 0.1178\n",
      "Epoch 135/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 135: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.1260 - val_loss: 0.1190\n",
      "Epoch 136/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 136: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 91s 244ms/step - loss: 0.1260 - val_loss: 0.1187\n",
      "Epoch 137/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 137: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 0.1260 - val_loss: 0.1179\n",
      "Epoch 138/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1258\n",
      "Epoch 138: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 91s 243ms/step - loss: 0.1258 - val_loss: 0.1184\n",
      "Epoch 139/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1256\n",
      "Epoch 139: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 91s 243ms/step - loss: 0.1256 - val_loss: 0.1179\n",
      "Epoch 140/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1252\n",
      "Epoch 140: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 92s 245ms/step - loss: 0.1252 - val_loss: 0.1184\n",
      "Epoch 141/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1252\n",
      "Epoch 141: val_loss did not improve from 0.11784\n",
      "375/375 [==============================] - 98s 262ms/step - loss: 0.1252 - val_loss: 0.1181\n",
      "Epoch 142/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1251\n",
      "Epoch 142: val_loss improved from 0.11784 to 0.11747, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 94s 249ms/step - loss: 0.1251 - val_loss: 0.1175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1250\n",
      "Epoch 143: val_loss did not improve from 0.11747\n",
      "375/375 [==============================] - 88s 234ms/step - loss: 0.1250 - val_loss: 0.1185\n",
      "Epoch 144/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1254\n",
      "Epoch 144: val_loss did not improve from 0.11747\n",
      "375/375 [==============================] - 86s 231ms/step - loss: 0.1254 - val_loss: 0.1177\n",
      "Epoch 145/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1248\n",
      "Epoch 145: val_loss did not improve from 0.11747\n",
      "375/375 [==============================] - 86s 228ms/step - loss: 0.1248 - val_loss: 0.1176\n",
      "Epoch 146/300\n",
      "374/375 [============================>.] - ETA: 5s - loss: 0.1244 \n",
      "Epoch 146: val_loss improved from 0.11747 to 0.11640, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 2043s 5s/step - loss: 0.1244 - val_loss: 0.1164\n",
      "Epoch 147/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 147: val_loss did not improve from 0.11640\n",
      "375/375 [==============================] - 20s 53ms/step - loss: 0.1244 - val_loss: 0.1177\n",
      "Epoch 148/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1240\n",
      "Epoch 148: val_loss did not improve from 0.11640\n",
      "375/375 [==============================] - 20s 54ms/step - loss: 0.1240 - val_loss: 0.1172\n",
      "Epoch 149/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 149: val_loss improved from 0.11640 to 0.11557, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 21s 56ms/step - loss: 0.1245 - val_loss: 0.1156\n",
      "Epoch 150/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 150: val_loss did not improve from 0.11557\n",
      "375/375 [==============================] - 36s 96ms/step - loss: 0.1242 - val_loss: 0.1160\n",
      "Epoch 151/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1241\n",
      "Epoch 151: val_loss did not improve from 0.11557\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.1241 - val_loss: 0.1173\n",
      "Epoch 152/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 152: val_loss improved from 0.11557 to 0.11533, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 72s 193ms/step - loss: 0.1237 - val_loss: 0.1153\n",
      "Epoch 153/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 153: val_loss did not improve from 0.11533\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.1237 - val_loss: 0.1160\n",
      "Epoch 154/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 154: val_loss did not improve from 0.11533\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 0.1234 - val_loss: 0.1179\n",
      "Epoch 155/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 155: val_loss did not improve from 0.11533\n",
      "375/375 [==============================] - 73s 196ms/step - loss: 0.1234 - val_loss: 0.1159\n",
      "Epoch 156/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 156: val_loss did not improve from 0.11533\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.1231 - val_loss: 0.1164\n",
      "Epoch 157/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 157: val_loss improved from 0.11533 to 0.11524, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 0.1230 - val_loss: 0.1152\n",
      "Epoch 158/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 158: val_loss did not improve from 0.11524\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 0.1230 - val_loss: 0.1173\n",
      "Epoch 159/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 159: val_loss improved from 0.11524 to 0.11454, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.1229 - val_loss: 0.1145\n",
      "Epoch 160/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 160: val_loss improved from 0.11454 to 0.11405, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1229 - val_loss: 0.1140\n",
      "Epoch 161/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 161: val_loss did not improve from 0.11405\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 0.1230 - val_loss: 0.1151\n",
      "Epoch 162/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1221\n",
      "Epoch 162: val_loss improved from 0.11405 to 0.11371, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1221 - val_loss: 0.1137\n",
      "Epoch 163/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1221\n",
      "Epoch 163: val_loss did not improve from 0.11371\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1221 - val_loss: 0.1150\n",
      "Epoch 164/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1219\n",
      "Epoch 164: val_loss did not improve from 0.11371\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 0.1219 - val_loss: 0.1159\n",
      "Epoch 165/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1216\n",
      "Epoch 165: val_loss improved from 0.11371 to 0.11361, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 0.1216 - val_loss: 0.1136\n",
      "Epoch 166/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 166: val_loss improved from 0.11361 to 0.11327, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 0.1215 - val_loss: 0.1133\n",
      "Epoch 167/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1217\n",
      "Epoch 167: val_loss did not improve from 0.11327\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.1217 - val_loss: 0.1142\n",
      "Epoch 168/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1210\n",
      "Epoch 168: val_loss improved from 0.11327 to 0.11286, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 0.1210 - val_loss: 0.1129\n",
      "Epoch 169/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1211\n",
      "Epoch 169: val_loss improved from 0.11286 to 0.11255, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 0.1211 - val_loss: 0.1126\n",
      "Epoch 170/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1209\n",
      "Epoch 170: val_loss improved from 0.11255 to 0.11209, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 0.1209 - val_loss: 0.1121\n",
      "Epoch 171/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1205\n",
      "Epoch 171: val_loss did not improve from 0.11209\n",
      "375/375 [==============================] - 49s 130ms/step - loss: 0.1205 - val_loss: 0.1140\n",
      "Epoch 172/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1204\n",
      "Epoch 172: val_loss did not improve from 0.11209\n",
      "375/375 [==============================] - 55s 146ms/step - loss: 0.1204 - val_loss: 0.1128\n",
      "Epoch 173/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1202\n",
      "Epoch 173: val_loss did not improve from 0.11209\n",
      "375/375 [==============================] - 80s 213ms/step - loss: 0.1202 - val_loss: 0.1123\n",
      "Epoch 174/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1198\n",
      "Epoch 174: val_loss did not improve from 0.11209\n",
      "375/375 [==============================] - 82s 219ms/step - loss: 0.1198 - val_loss: 0.1145\n",
      "Epoch 175/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1199\n",
      "Epoch 175: val_loss did not improve from 0.11209\n",
      "375/375 [==============================] - 81s 215ms/step - loss: 0.1199 - val_loss: 0.1128\n",
      "Epoch 176/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 176: val_loss improved from 0.11209 to 0.11085, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 80s 213ms/step - loss: 0.1196 - val_loss: 0.1108\n",
      "Epoch 177/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 177: val_loss did not improve from 0.11085\n",
      "375/375 [==============================] - 79s 211ms/step - loss: 0.1191 - val_loss: 0.1111\n",
      "Epoch 178/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1194\n",
      "Epoch 178: val_loss improved from 0.11085 to 0.11013, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 0.1194 - val_loss: 0.1101\n",
      "Epoch 179/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 179: val_loss did not improve from 0.11013\n",
      "375/375 [==============================] - 80s 212ms/step - loss: 0.1191 - val_loss: 0.1113\n",
      "Epoch 180/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 180: val_loss did not improve from 0.11013\n",
      "375/375 [==============================] - 77s 205ms/step - loss: 0.1187 - val_loss: 0.1105\n",
      "Epoch 181/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 181: val_loss improved from 0.11013 to 0.10990, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 76s 204ms/step - loss: 0.1187 - val_loss: 0.1099\n",
      "Epoch 182/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 182: val_loss improved from 0.10990 to 0.10980, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1184 - val_loss: 0.1098\n",
      "Epoch 183/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 183: val_loss did not improve from 0.10980\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 0.1181 - val_loss: 0.1100\n",
      "Epoch 184/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1180\n",
      "Epoch 184: val_loss did not improve from 0.10980\n",
      "375/375 [==============================] - 66s 175ms/step - loss: 0.1180 - val_loss: 0.1099\n",
      "Epoch 185/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 185: val_loss improved from 0.10980 to 0.10896, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.1170 - val_loss: 0.1090\n",
      "Epoch 186/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 186: val_loss did not improve from 0.10896\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 0.1170 - val_loss: 0.1098\n",
      "Epoch 187/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 187: val_loss improved from 0.10896 to 0.10822, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 0.1173 - val_loss: 0.1082\n",
      "Epoch 188/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 188: val_loss improved from 0.10822 to 0.10698, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 0.1166 - val_loss: 0.1070\n",
      "Epoch 189/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 189: val_loss did not improve from 0.10698\n",
      "375/375 [==============================] - 74s 196ms/step - loss: 0.1161 - val_loss: 0.1092\n",
      "Epoch 190/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1165\n",
      "Epoch 190: val_loss did not improve from 0.10698\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.1165 - val_loss: 0.1089\n",
      "Epoch 191/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 191: val_loss improved from 0.10698 to 0.10693, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 37s 100ms/step - loss: 0.1158 - val_loss: 0.1069\n",
      "Epoch 192/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1153\n",
      "Epoch 192: val_loss did not improve from 0.10693\n",
      "375/375 [==============================] - 35s 92ms/step - loss: 0.1153 - val_loss: 0.1075\n",
      "Epoch 193/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 193: val_loss improved from 0.10693 to 0.10567, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 30s 81ms/step - loss: 0.1156 - val_loss: 0.1057\n",
      "Epoch 194/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 194: val_loss did not improve from 0.10567\n",
      "375/375 [==============================] - 110s 294ms/step - loss: 0.1154 - val_loss: 0.1064\n",
      "Epoch 195/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 195: val_loss did not improve from 0.10567\n",
      "375/375 [==============================] - 98s 262ms/step - loss: 0.1149 - val_loss: 0.1064\n",
      "Epoch 196/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 196: val_loss did not improve from 0.10567\n",
      "375/375 [==============================] - 96s 257ms/step - loss: 0.1144 - val_loss: 0.1075\n",
      "Epoch 197/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 197: val_loss did not improve from 0.10567\n",
      "375/375 [==============================] - 98s 262ms/step - loss: 0.1146 - val_loss: 0.1062\n",
      "Epoch 198/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 198: val_loss improved from 0.10567 to 0.10565, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 91s 243ms/step - loss: 0.1139 - val_loss: 0.1056\n",
      "Epoch 199/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 199: val_loss improved from 0.10565 to 0.10433, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 94s 252ms/step - loss: 0.1136 - val_loss: 0.1043\n",
      "Epoch 200/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 200: val_loss did not improve from 0.10433\n",
      "375/375 [==============================] - 102s 272ms/step - loss: 0.1132 - val_loss: 0.1055\n",
      "Epoch 201/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 201: val_loss did not improve from 0.10433\n",
      "375/375 [==============================] - 93s 248ms/step - loss: 0.1129 - val_loss: 0.1071\n",
      "Epoch 202/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 202: val_loss did not improve from 0.10433\n",
      "375/375 [==============================] - 89s 237ms/step - loss: 0.1130 - val_loss: 0.1053\n",
      "Epoch 203/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 203: val_loss did not improve from 0.10433\n",
      "375/375 [==============================] - 89s 236ms/step - loss: 0.1126 - val_loss: 0.1045\n",
      "Epoch 204/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 204: val_loss improved from 0.10433 to 0.10323, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.1126 - val_loss: 0.1032\n",
      "Epoch 205/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 205: val_loss did not improve from 0.10323\n",
      "375/375 [==============================] - 88s 234ms/step - loss: 0.1119 - val_loss: 0.1055\n",
      "Epoch 206/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 206: val_loss improved from 0.10323 to 0.10202, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.1119 - val_loss: 0.1020\n",
      "Epoch 207/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 207: val_loss did not improve from 0.10202\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 0.1113 - val_loss: 0.1037\n",
      "Epoch 208/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 208: val_loss did not improve from 0.10202\n",
      "375/375 [==============================] - 85s 225ms/step - loss: 0.1108 - val_loss: 0.1030\n",
      "Epoch 209/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 209: val_loss did not improve from 0.10202\n",
      "375/375 [==============================] - 89s 236ms/step - loss: 0.1110 - val_loss: 0.1025\n",
      "Epoch 210/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 210: val_loss improved from 0.10202 to 0.10181, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.1104 - val_loss: 0.1018\n",
      "Epoch 211/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 211: val_loss improved from 0.10181 to 0.10065, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1104 - val_loss: 0.1006\n",
      "Epoch 212/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 212: val_loss did not improve from 0.10065\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.1098 - val_loss: 0.1010\n",
      "Epoch 213/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 213: val_loss improved from 0.10065 to 0.10033, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 82s 219ms/step - loss: 0.1101 - val_loss: 0.1003\n",
      "Epoch 214/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 214: val_loss improved from 0.10033 to 0.09926, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.1095 - val_loss: 0.0993\n",
      "Epoch 215/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 215: val_loss did not improve from 0.09926\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.1093 - val_loss: 0.1011\n",
      "Epoch 216/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 216: val_loss improved from 0.09926 to 0.09921, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 85s 227ms/step - loss: 0.1085 - val_loss: 0.0992\n",
      "Epoch 217/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 217: val_loss improved from 0.09921 to 0.09893, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.1083 - val_loss: 0.0989\n",
      "Epoch 218/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 218: val_loss did not improve from 0.09893\n",
      "375/375 [==============================] - 96s 255ms/step - loss: 0.1084 - val_loss: 0.0995\n",
      "Epoch 219/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 219: val_loss did not improve from 0.09893\n",
      "375/375 [==============================] - 89s 236ms/step - loss: 0.1077 - val_loss: 0.0996\n",
      "Epoch 220/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 220: val_loss improved from 0.09893 to 0.09818, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 111s 295ms/step - loss: 0.1073 - val_loss: 0.0982\n",
      "Epoch 221/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 221: val_loss did not improve from 0.09818\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.1072 - val_loss: 0.0992\n",
      "Epoch 222/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 222: val_loss improved from 0.09818 to 0.09720, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 93s 249ms/step - loss: 0.1067 - val_loss: 0.0972\n",
      "Epoch 223/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 223: val_loss did not improve from 0.09720\n",
      "375/375 [==============================] - 92s 245ms/step - loss: 0.1062 - val_loss: 0.0983\n",
      "Epoch 224/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 224: val_loss improved from 0.09720 to 0.09718, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 97s 260ms/step - loss: 0.1066 - val_loss: 0.0972\n",
      "Epoch 225/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 225: val_loss improved from 0.09718 to 0.09660, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 97s 258ms/step - loss: 0.1057 - val_loss: 0.0966\n",
      "Epoch 226/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 226: val_loss did not improve from 0.09660\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.1060 - val_loss: 0.0967\n",
      "Epoch 227/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 227: val_loss did not improve from 0.09660\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.1052 - val_loss: 0.0989\n",
      "Epoch 228/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 228: val_loss improved from 0.09660 to 0.09516, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 93s 248ms/step - loss: 0.1053 - val_loss: 0.0952\n",
      "Epoch 229/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 229: val_loss did not improve from 0.09516\n",
      "375/375 [==============================] - 95s 253ms/step - loss: 0.1050 - val_loss: 0.0978\n",
      "Epoch 230/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 230: val_loss did not improve from 0.09516\n",
      "375/375 [==============================] - 83s 222ms/step - loss: 0.1046 - val_loss: 0.0953\n",
      "Epoch 231/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 231: val_loss did not improve from 0.09516\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.1048 - val_loss: 0.0959\n",
      "Epoch 232/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 232: val_loss improved from 0.09516 to 0.09417, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 87s 231ms/step - loss: 0.1042 - val_loss: 0.0942\n",
      "Epoch 233/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 233: val_loss improved from 0.09417 to 0.09386, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 106s 282ms/step - loss: 0.1034 - val_loss: 0.0939\n",
      "Epoch 234/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 234: val_loss improved from 0.09386 to 0.09357, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 94s 251ms/step - loss: 0.1037 - val_loss: 0.0936\n",
      "Epoch 235/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 235: val_loss did not improve from 0.09357\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 0.1033 - val_loss: 0.0940\n",
      "Epoch 236/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 236: val_loss did not improve from 0.09357\n",
      "375/375 [==============================] - 90s 240ms/step - loss: 0.1027 - val_loss: 0.0942\n",
      "Epoch 237/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 237: val_loss did not improve from 0.09357\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.1024 - val_loss: 0.0938\n",
      "Epoch 238/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 238: val_loss did not improve from 0.09357\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 0.1024 - val_loss: 0.0955\n",
      "Epoch 239/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 239: val_loss improved from 0.09357 to 0.09166, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 90s 239ms/step - loss: 0.1020 - val_loss: 0.0917\n",
      "Epoch 240/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 240: val_loss did not improve from 0.09166\n",
      "375/375 [==============================] - 90s 240ms/step - loss: 0.1018 - val_loss: 0.0960\n",
      "Epoch 241/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 241: val_loss did not improve from 0.09166\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.1018 - val_loss: 0.0933\n",
      "Epoch 242/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 242: val_loss improved from 0.09166 to 0.09101, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.1012 - val_loss: 0.0910\n",
      "Epoch 243/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 243: val_loss did not improve from 0.09101\n",
      "375/375 [==============================] - 89s 237ms/step - loss: 0.1010 - val_loss: 0.0938\n",
      "Epoch 244/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 244: val_loss did not improve from 0.09101\n",
      "375/375 [==============================] - 96s 255ms/step - loss: 0.1009 - val_loss: 0.0925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 245: val_loss did not improve from 0.09101\n",
      "375/375 [==============================] - 94s 252ms/step - loss: 0.1003 - val_loss: 0.0914\n",
      "Epoch 246/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 246: val_loss did not improve from 0.09101\n",
      "375/375 [==============================] - 94s 251ms/step - loss: 0.1003 - val_loss: 0.0928\n",
      "Epoch 247/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 247: val_loss improved from 0.09101 to 0.09071, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 89s 239ms/step - loss: 0.1001 - val_loss: 0.0907\n",
      "Epoch 248/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 248: val_loss improved from 0.09071 to 0.08956, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 88s 236ms/step - loss: 0.0997 - val_loss: 0.0896\n",
      "Epoch 249/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 249: val_loss did not improve from 0.08956\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.0997 - val_loss: 0.0935\n",
      "Epoch 250/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 250: val_loss did not improve from 0.08956\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.0997 - val_loss: 0.0906\n",
      "Epoch 251/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 251: val_loss did not improve from 0.08956\n",
      "375/375 [==============================] - 79s 212ms/step - loss: 0.0989 - val_loss: 0.0909\n",
      "Epoch 252/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0986\n",
      "Epoch 252: val_loss improved from 0.08956 to 0.08920, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 0.0986 - val_loss: 0.0892\n",
      "Epoch 253/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 253: val_loss improved from 0.08920 to 0.08919, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 87s 231ms/step - loss: 0.0986 - val_loss: 0.0892\n",
      "Epoch 254/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 254: val_loss did not improve from 0.08919\n",
      "375/375 [==============================] - 95s 255ms/step - loss: 0.0986 - val_loss: 0.0898\n",
      "Epoch 255/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 255: val_loss did not improve from 0.08919\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.0981 - val_loss: 0.0893\n",
      "Epoch 256/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 256: val_loss did not improve from 0.08919\n",
      "375/375 [==============================] - 87s 233ms/step - loss: 0.0983 - val_loss: 0.0903\n",
      "Epoch 257/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 257: val_loss improved from 0.08919 to 0.08882, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 0.0976 - val_loss: 0.0888\n",
      "Epoch 258/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 258: val_loss improved from 0.08882 to 0.08854, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 88s 236ms/step - loss: 0.0977 - val_loss: 0.0885\n",
      "Epoch 259/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 259: val_loss did not improve from 0.08854\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.0975 - val_loss: 0.0904\n",
      "Epoch 260/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 260: val_loss improved from 0.08854 to 0.08839, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 89s 237ms/step - loss: 0.0978 - val_loss: 0.0884\n",
      "Epoch 261/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 261: val_loss did not improve from 0.08839\n",
      "375/375 [==============================] - 96s 257ms/step - loss: 0.0969 - val_loss: 0.0893\n",
      "Epoch 262/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 262: val_loss did not improve from 0.08839\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.0973 - val_loss: 0.0892\n",
      "Epoch 263/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 263: val_loss improved from 0.08839 to 0.08806, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 93s 247ms/step - loss: 0.0970 - val_loss: 0.0881\n",
      "Epoch 264/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 264: val_loss improved from 0.08806 to 0.08702, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.0967 - val_loss: 0.0870\n",
      "Epoch 265/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 265: val_loss did not improve from 0.08702\n",
      "375/375 [==============================] - 87s 233ms/step - loss: 0.0965 - val_loss: 0.0873\n",
      "Epoch 266/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0961\n",
      "Epoch 266: val_loss did not improve from 0.08702\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.0961 - val_loss: 0.0873\n",
      "Epoch 267/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0963\n",
      "Epoch 267: val_loss improved from 0.08702 to 0.08644, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 0.0963 - val_loss: 0.0864\n",
      "Epoch 268/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0959\n",
      "Epoch 268: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.0959 - val_loss: 0.0868\n",
      "Epoch 269/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0958\n",
      "Epoch 269: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 84s 223ms/step - loss: 0.0958 - val_loss: 0.0877\n",
      "Epoch 270/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0957\n",
      "Epoch 270: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0957 - val_loss: 0.0872\n",
      "Epoch 271/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0952\n",
      "Epoch 271: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 86s 228ms/step - loss: 0.0952 - val_loss: 0.0875\n",
      "Epoch 272/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 272: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.0954 - val_loss: 0.0893\n",
      "Epoch 273/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0945\n",
      "Epoch 273: val_loss did not improve from 0.08644\n",
      "375/375 [==============================] - 87s 232ms/step - loss: 0.0945 - val_loss: 0.0868\n",
      "Epoch 274/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 274: val_loss improved from 0.08644 to 0.08576, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 91s 241ms/step - loss: 0.0948 - val_loss: 0.0858\n",
      "Epoch 275/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0951\n",
      "Epoch 275: val_loss did not improve from 0.08576\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0951 - val_loss: 0.0864\n",
      "Epoch 276/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 276: val_loss did not improve from 0.08576\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.0944 - val_loss: 0.0870\n",
      "Epoch 277/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0944\n",
      "Epoch 277: val_loss did not improve from 0.08576\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.0944 - val_loss: 0.0864\n",
      "Epoch 278/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0943\n",
      "Epoch 278: val_loss did not improve from 0.08576\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0943 - val_loss: 0.0869\n",
      "Epoch 279/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 279: val_loss improved from 0.08576 to 0.08476, saving model to transformer_model_updated.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 86s 228ms/step - loss: 0.0942 - val_loss: 0.0848\n",
      "Epoch 280/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0941\n",
      "Epoch 280: val_loss did not improve from 0.08476\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0941 - val_loss: 0.0856\n",
      "Epoch 281/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0942\n",
      "Epoch 281: val_loss did not improve from 0.08476\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.0942 - val_loss: 0.0868\n",
      "Epoch 282/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0934\n",
      "Epoch 282: val_loss did not improve from 0.08476\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.0934 - val_loss: 0.0862\n",
      "Epoch 283/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 283: val_loss did not improve from 0.08476\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.0938 - val_loss: 0.0864\n",
      "Epoch 284/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 284: val_loss did not improve from 0.08476\n",
      "375/375 [==============================] - 82s 219ms/step - loss: 0.0933 - val_loss: 0.0848\n",
      "Epoch 285/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0938\n",
      "Epoch 285: val_loss improved from 0.08476 to 0.08406, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 83s 222ms/step - loss: 0.0938 - val_loss: 0.0841\n",
      "Epoch 286/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0933\n",
      "Epoch 286: val_loss did not improve from 0.08406\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.0933 - val_loss: 0.0853\n",
      "Epoch 287/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 287: val_loss did not improve from 0.08406\n",
      "375/375 [==============================] - 91s 243ms/step - loss: 0.0931 - val_loss: 0.0853\n",
      "Epoch 288/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0930\n",
      "Epoch 288: val_loss improved from 0.08406 to 0.08368, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 87s 233ms/step - loss: 0.0930 - val_loss: 0.0837\n",
      "Epoch 289/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 289: val_loss did not improve from 0.08368\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.0927 - val_loss: 0.0842\n",
      "Epoch 290/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0928\n",
      "Epoch 290: val_loss improved from 0.08368 to 0.08332, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 86s 230ms/step - loss: 0.0928 - val_loss: 0.0833\n",
      "Epoch 291/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 291: val_loss did not improve from 0.08332\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.0921 - val_loss: 0.0870\n",
      "Epoch 292/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0925\n",
      "Epoch 292: val_loss did not improve from 0.08332\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0925 - val_loss: 0.0857\n",
      "Epoch 293/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0925\n",
      "Epoch 293: val_loss did not improve from 0.08332\n",
      "375/375 [==============================] - 84s 223ms/step - loss: 0.0925 - val_loss: 0.0838\n",
      "Epoch 294/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0921\n",
      "Epoch 294: val_loss improved from 0.08332 to 0.08312, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 85s 227ms/step - loss: 0.0921 - val_loss: 0.0831\n",
      "Epoch 295/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 295: val_loss improved from 0.08312 to 0.08268, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 84s 223ms/step - loss: 0.0922 - val_loss: 0.0827\n",
      "Epoch 296/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0923\n",
      "Epoch 296: val_loss did not improve from 0.08268\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0923 - val_loss: 0.0840\n",
      "Epoch 297/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0917\n",
      "Epoch 297: val_loss did not improve from 0.08268\n",
      "375/375 [==============================] - 86s 229ms/step - loss: 0.0917 - val_loss: 0.0837\n",
      "Epoch 298/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0916\n",
      "Epoch 298: val_loss improved from 0.08268 to 0.08243, saving model to transformer_model_updated.h5\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0916 - val_loss: 0.0824\n",
      "Epoch 299/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0916\n",
      "Epoch 299: val_loss did not improve from 0.08243\n",
      "375/375 [==============================] - 85s 226ms/step - loss: 0.0916 - val_loss: 0.0858\n",
      "Epoch 300/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 300: val_loss did not improve from 0.08243\n",
      "375/375 [==============================] - 85s 227ms/step - loss: 0.0914 - val_loss: 0.0837\n"
     ]
    }
   ],
   "source": [
    "# # Callbacks for early stopping and saving the best model\n",
    "# callback = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1, mode=\"min\")\n",
    "# checkpoint = ModelCheckpoint(filepath='transformer_model_updated.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# # Assume you have your training and validation data loaded: x1_train, y1_train, x1_val, y1_val\n",
    "# # Continue training the model\n",
    "# history = model.fit(x1_train, y1_train, callbacks=[callback, checkpoint], batch_size=32, epochs=300, verbose=1, validation_data=(x1_val, y1_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9359327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Save the training history\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "# hist_df.to_csv('history_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ce89d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 1: val_loss improved from inf to 0.07044, saving model to transformer_model_updated_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mali19\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 96s 241ms/step - loss: 0.0785 - val_loss: 0.0704\n",
      "Epoch 2/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 2: val_loss improved from 0.07044 to 0.07031, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 95s 254ms/step - loss: 0.0784 - val_loss: 0.0703\n",
      "Epoch 3/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 3: val_loss improved from 0.07031 to 0.06992, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 94s 251ms/step - loss: 0.0787 - val_loss: 0.0699\n",
      "Epoch 4/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0786\n",
      "Epoch 4: val_loss improved from 0.06992 to 0.06943, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 95s 253ms/step - loss: 0.0786 - val_loss: 0.0694\n",
      "Epoch 5/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0788\n",
      "Epoch 5: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 104s 278ms/step - loss: 0.0788 - val_loss: 0.0700\n",
      "Epoch 6/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 6: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 102s 272ms/step - loss: 0.0785 - val_loss: 0.0707\n",
      "Epoch 7/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 7: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 92s 247ms/step - loss: 0.0787 - val_loss: 0.0703\n",
      "Epoch 8/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 8: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 103s 275ms/step - loss: 0.0785 - val_loss: 0.0727\n",
      "Epoch 9/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 9: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 96s 255ms/step - loss: 0.0787 - val_loss: 0.0718\n",
      "Epoch 10/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 10: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 95s 252ms/step - loss: 0.0783 - val_loss: 0.0698\n",
      "Epoch 11/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0786\n",
      "Epoch 11: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 93s 247ms/step - loss: 0.0786 - val_loss: 0.0713\n",
      "Epoch 12/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0787\n",
      "Epoch 12: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 96s 255ms/step - loss: 0.0787 - val_loss: 0.0697\n",
      "Epoch 13/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0786\n",
      "Epoch 13: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 102s 273ms/step - loss: 0.0786 - val_loss: 0.0703\n",
      "Epoch 14/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0778\n",
      "Epoch 14: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 105s 281ms/step - loss: 0.0778 - val_loss: 0.0701\n",
      "Epoch 15/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 15: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 105s 280ms/step - loss: 0.0784 - val_loss: 0.0708\n",
      "Epoch 16/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 16: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 112s 300ms/step - loss: 0.0784 - val_loss: 0.0710\n",
      "Epoch 17/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 17: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 99s 265ms/step - loss: 0.0785 - val_loss: 0.0713\n",
      "Epoch 18/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 18: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 104s 278ms/step - loss: 0.0784 - val_loss: 0.0701\n",
      "Epoch 19/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 19: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 111s 296ms/step - loss: 0.0782 - val_loss: 0.0722\n",
      "Epoch 20/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0786\n",
      "Epoch 20: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 101s 270ms/step - loss: 0.0786 - val_loss: 0.0697\n",
      "Epoch 21/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 21: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 105s 281ms/step - loss: 0.0783 - val_loss: 0.0712\n",
      "Epoch 22/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 22: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 113s 301ms/step - loss: 0.0784 - val_loss: 0.0695\n",
      "Epoch 23/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 23: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 165s 440ms/step - loss: 0.0784 - val_loss: 0.0699\n",
      "Epoch 24/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 24: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 125s 333ms/step - loss: 0.0783 - val_loss: 0.0699\n",
      "Epoch 25/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 25: val_loss did not improve from 0.06943\n",
      "375/375 [==============================] - 123s 329ms/step - loss: 0.0782 - val_loss: 0.0703\n",
      "Epoch 26/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0785\n",
      "Epoch 26: val_loss improved from 0.06943 to 0.06909, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 114s 303ms/step - loss: 0.0785 - val_loss: 0.0691\n",
      "Epoch 27/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 27: val_loss did not improve from 0.06909\n",
      "375/375 [==============================] - 91s 244ms/step - loss: 0.0783 - val_loss: 0.0691\n",
      "Epoch 28/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 28: val_loss did not improve from 0.06909\n",
      "375/375 [==============================] - 95s 254ms/step - loss: 0.0782 - val_loss: 0.0695\n",
      "Epoch 29/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 29: val_loss did not improve from 0.06909\n",
      "375/375 [==============================] - 97s 260ms/step - loss: 0.0783 - val_loss: 0.0710\n",
      "Epoch 30/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 30: val_loss did not improve from 0.06909\n",
      "375/375 [==============================] - 93s 250ms/step - loss: 0.0782 - val_loss: 0.0698\n",
      "Epoch 31/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 31: val_loss improved from 0.06909 to 0.06888, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 92s 244ms/step - loss: 0.0780 - val_loss: 0.0689\n",
      "Epoch 32/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0784\n",
      "Epoch 32: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 90s 240ms/step - loss: 0.0784 - val_loss: 0.0713\n",
      "Epoch 33/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0781\n",
      "Epoch 33: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 90s 240ms/step - loss: 0.0781 - val_loss: 0.0697\n",
      "Epoch 34/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 34: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 88s 235ms/step - loss: 0.0780 - val_loss: 0.0700\n",
      "Epoch 35/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 35: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 106s 282ms/step - loss: 0.0777 - val_loss: 0.0691\n",
      "Epoch 36/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 36: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 98s 262ms/step - loss: 0.0779 - val_loss: 0.0695\n",
      "Epoch 37/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 37: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 99s 265ms/step - loss: 0.0782 - val_loss: 0.0694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0782\n",
      "Epoch 38: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 56s 149ms/step - loss: 0.0782 - val_loss: 0.0689\n",
      "Epoch 39/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0781\n",
      "Epoch 39: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 54s 145ms/step - loss: 0.0781 - val_loss: 0.0692\n",
      "Epoch 40/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0781\n",
      "Epoch 40: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 55s 147ms/step - loss: 0.0781 - val_loss: 0.0700\n",
      "Epoch 41/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0780\n",
      "Epoch 41: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 66s 175ms/step - loss: 0.0780 - val_loss: 0.0700\n",
      "Epoch 42/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 42: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 61s 163ms/step - loss: 0.0777 - val_loss: 0.0711\n",
      "Epoch 43/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0781\n",
      "Epoch 43: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 44s 118ms/step - loss: 0.0781 - val_loss: 0.0722\n",
      "Epoch 44/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0782\n",
      "Epoch 44: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 44s 117ms/step - loss: 0.0782 - val_loss: 0.0712\n",
      "Epoch 45/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 45: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 60s 161ms/step - loss: 0.0776 - val_loss: 0.0711\n",
      "Epoch 46/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 46: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 84s 224ms/step - loss: 0.0776 - val_loss: 0.0703\n",
      "Epoch 47/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0781\n",
      "Epoch 47: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.0781 - val_loss: 0.0697\n",
      "Epoch 48/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 48: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 90s 240ms/step - loss: 0.0779 - val_loss: 0.0700\n",
      "Epoch 49/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 49: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.0779 - val_loss: 0.0698\n",
      "Epoch 50/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0780\n",
      "Epoch 50: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 80s 214ms/step - loss: 0.0780 - val_loss: 0.0689\n",
      "Epoch 51/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 51: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 91s 243ms/step - loss: 0.0777 - val_loss: 0.0701\n",
      "Epoch 52/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 52: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 98s 261ms/step - loss: 0.0783 - val_loss: 0.0709\n",
      "Epoch 53/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 53: val_loss did not improve from 0.06888\n",
      "375/375 [==============================] - 91s 244ms/step - loss: 0.0779 - val_loss: 0.0691\n",
      "Epoch 54/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0779\n",
      "Epoch 54: val_loss improved from 0.06888 to 0.06827, saving model to transformer_model_updated_2.h5\n",
      "375/375 [==============================] - 64s 169ms/step - loss: 0.0779 - val_loss: 0.0683\n",
      "Epoch 55/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0778\n",
      "Epoch 55: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 36s 97ms/step - loss: 0.0778 - val_loss: 0.0688\n",
      "Epoch 56/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0778\n",
      "Epoch 56: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 37s 100ms/step - loss: 0.0778 - val_loss: 0.0700\n",
      "Epoch 57/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0781\n",
      "Epoch 57: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 48s 127ms/step - loss: 0.0781 - val_loss: 0.0691\n",
      "Epoch 58/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0775\n",
      "Epoch 58: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 46s 121ms/step - loss: 0.0775 - val_loss: 0.0691\n",
      "Epoch 59/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0774\n",
      "Epoch 59: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 54s 143ms/step - loss: 0.0774 - val_loss: 0.0693\n",
      "Epoch 60/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 60: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 63s 167ms/step - loss: 0.0779 - val_loss: 0.0692\n",
      "Epoch 61/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0775\n",
      "Epoch 61: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 58s 155ms/step - loss: 0.0775 - val_loss: 0.0706\n",
      "Epoch 62/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 62: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 77s 206ms/step - loss: 0.0776 - val_loss: 0.0706\n",
      "Epoch 63/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 63: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 82s 218ms/step - loss: 0.0774 - val_loss: 0.0688\n",
      "Epoch 64/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 64: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 82s 220ms/step - loss: 0.0773 - val_loss: 0.0693\n",
      "Epoch 65/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 65: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 84s 225ms/step - loss: 0.0779 - val_loss: 0.0688\n",
      "Epoch 66/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0774\n",
      "Epoch 66: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 82s 219ms/step - loss: 0.0774 - val_loss: 0.0687\n",
      "Epoch 67/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0778\n",
      "Epoch 67: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 66s 175ms/step - loss: 0.0778 - val_loss: 0.0689\n",
      "Epoch 68/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 68: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 0.0774 - val_loss: 0.0692\n",
      "Epoch 69/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 69: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 83s 220ms/step - loss: 0.0776 - val_loss: 0.0694\n",
      "Epoch 70/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0778\n",
      "Epoch 70: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 79s 210ms/step - loss: 0.0778 - val_loss: 0.0689\n",
      "Epoch 71/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0776\n",
      "Epoch 71: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 48s 128ms/step - loss: 0.0775 - val_loss: 0.0687\n",
      "Epoch 72/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 72: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.0773 - val_loss: 0.0705\n",
      "Epoch 73/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 73: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 67s 178ms/step - loss: 0.0772 - val_loss: 0.0700\n",
      "Epoch 74/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0775\n",
      "Epoch 74: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 0.0775 - val_loss: 0.0699\n",
      "Epoch 75/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 75: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 80s 212ms/step - loss: 0.0774 - val_loss: 0.0704\n",
      "Epoch 76/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 76: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 75s 200ms/step - loss: 0.0773 - val_loss: 0.0690\n",
      "Epoch 77/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0780\n",
      "Epoch 77: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 59s 156ms/step - loss: 0.0780 - val_loss: 0.0695\n",
      "Epoch 78/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 78: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 45s 120ms/step - loss: 0.0772 - val_loss: 0.0683\n",
      "Epoch 79/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 79: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 0.0774 - val_loss: 0.0689\n",
      "Epoch 80/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 80: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 91s 242ms/step - loss: 0.0772 - val_loss: 0.0694\n",
      "Epoch 81/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 81: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 74s 197ms/step - loss: 0.0772 - val_loss: 0.0689\n",
      "Epoch 82/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 82: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 83s 223ms/step - loss: 0.0776 - val_loss: 0.0689\n",
      "Epoch 83/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 83: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 79s 210ms/step - loss: 0.0776 - val_loss: 0.0692\n",
      "Epoch 84/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0778\n",
      "Epoch 84: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 66s 175ms/step - loss: 0.0778 - val_loss: 0.0687\n",
      "Epoch 85/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0771\n",
      "Epoch 85: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 65s 173ms/step - loss: 0.0771 - val_loss: 0.0689\n",
      "Epoch 86/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 86: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 63s 168ms/step - loss: 0.0770 - val_loss: 0.0685\n",
      "Epoch 87/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 87: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.0773 - val_loss: 0.0699\n",
      "Epoch 88/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0775\n",
      "Epoch 88: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 60s 160ms/step - loss: 0.0775 - val_loss: 0.0693\n",
      "Epoch 89/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 89: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 60s 159ms/step - loss: 0.0774 - val_loss: 0.0686\n",
      "Epoch 90/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 90: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 0.0776 - val_loss: 0.0688\n",
      "Epoch 91/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0770\n",
      "Epoch 91: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 79s 212ms/step - loss: 0.0770 - val_loss: 0.0696\n",
      "Epoch 92/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0772\n",
      "Epoch 92: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 0.0772 - val_loss: 0.0726\n",
      "Epoch 93/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0776\n",
      "Epoch 93: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 0.0776 - val_loss: 0.0684\n",
      "Epoch 94/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0775\n",
      "Epoch 94: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 59s 157ms/step - loss: 0.0775 - val_loss: 0.0687\n",
      "Epoch 95/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0769\n",
      "Epoch 95: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 47s 125ms/step - loss: 0.0769 - val_loss: 0.0689\n",
      "Epoch 96/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0769\n",
      "Epoch 96: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 54s 144ms/step - loss: 0.0769 - val_loss: 0.0703\n",
      "Epoch 97/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0770\n",
      "Epoch 97: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 41s 110ms/step - loss: 0.0770 - val_loss: 0.0683\n",
      "Epoch 98/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0775\n",
      "Epoch 98: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 47s 125ms/step - loss: 0.0775 - val_loss: 0.0698\n",
      "Epoch 99/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0772\n",
      "Epoch 99: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 58s 155ms/step - loss: 0.0772 - val_loss: 0.0684\n",
      "Epoch 100/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 100: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 78s 207ms/step - loss: 0.0773 - val_loss: 0.0689\n",
      "Epoch 101/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0772\n",
      "Epoch 101: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 63s 167ms/step - loss: 0.0772 - val_loss: 0.0686\n",
      "Epoch 102/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 102: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 81s 216ms/step - loss: 0.0773 - val_loss: 0.0693\n",
      "Epoch 103/300\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0773\n",
      "Epoch 103: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 0.0773 - val_loss: 0.0684\n",
      "Epoch 104/300\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 104: val_loss did not improve from 0.06827\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 0.0768 - val_loss: 0.0691\n",
      "Epoch 104: early stopping\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the previously saved model\n",
    "# model_path = 'transformer_model_updated_1.h5'\n",
    "# model = load_model(model_path)\n",
    "\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Re-compile the model with potentially adjusted learning rate or optimizer\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001), loss='mape')\n",
    "\n",
    "# # Callbacks for early stopping and saving the best model\n",
    "# callback = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1, mode=\"min\")\n",
    "# checkpoint = ModelCheckpoint(filepath='transformer_model_updated_2.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# # Assume you have your training and validation data loaded: x1_train, y1_train, x1_val, y1_val\n",
    "# # Continue training the model\n",
    "# history = model.fit(x1_train, y1_train, callbacks=[callback, checkpoint], batch_size=32, epochs=300, verbose=1, validation_data=(x1_val, y1_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e1603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Save the training history\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "# hist_df.to_csv('history_updated_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6065ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import backend\n",
    "from matplotlib import pyplot as plt\n",
    "import joblib\n",
    "\n",
    "samples = 200000\n",
    "frames_per_file = 1\n",
    "seq_length = 25 #sequence window to be used\n",
    "steps_to_pred = 50000 #number of steps ahead that want to be predicted\n",
    "batch_size = 128\n",
    "molecules = 2\n",
    "species = 'hmf'\n",
    "# horizons = np.array([1, 50, 100, 200, 300])\n",
    "horizons = np.array([100,100000])\n",
    "downsample_factor = 10\n",
    "samples = int(samples/downsample_factor)\n",
    "steps_to_pred = int(steps_to_pred/downsample_factor)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(samples-seq_length):\n",
    "        x = data[i:(i+seq_length), :]\n",
    "        y = data[i+seq_length, :]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2770253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 6)\n"
     ]
    }
   ],
   "source": [
    "data_1 = np.load('C:/Users/mali19/Downloads/KevinCode/inputs/input_latest/%s_data.npy' %species)\n",
    "data = np.add(data_1, 3.549)\n",
    "data = data[::downsample_factor, :]\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6526f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_, output = create_sequences(data, seq_length) #(samples-seq_length) x seq_length x 8000, (samples-seq_length) x 8000 tensor\n",
    "\n",
    "# model = load_model('transformer_model_%s.h5' % species)\n",
    "\n",
    "model = load_model('transformer_model_hmf.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2dd7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [5000,10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e58f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 predicted\n",
      "Step 1000 predicted\n",
      "Step 2000 predicted\n",
      "Step 3000 predicted\n",
      "Step 4000 predicted\n",
      "Step 0 predicted\n",
      "Step 1000 predicted\n",
      "Step 2000 predicted\n",
      "Step 3000 predicted\n",
      "Step 4000 predicted\n"
     ]
    }
   ],
   "source": [
    "for j in range(2):\n",
    "    horizon = horizons[j]\n",
    "\n",
    "    #(samples-seq_length) x seq_length x 8002, (samples-seq_length) x 8001 tensor\n",
    "    test = np.expand_dims(input_[-steps_to_pred, :, :], axis=0)\n",
    "\n",
    "    performance = np.zeros((steps_to_pred, 1))\n",
    "    predictions = np.zeros((steps_to_pred, 2*molecules*3))\n",
    "\n",
    "    for i in range(steps_to_pred):\n",
    "        prediction = model.predict(test, steps=1, verbose=0)\n",
    "        temp = np.expand_dims(output[-steps_to_pred+i, :], axis=0)\n",
    "\n",
    "        performance[i, 0] = tf.metrics.mean_absolute_percentage_error(temp, prediction)\n",
    "\n",
    "        predictions[i, 0:molecules*3] = np.expand_dims(prediction, axis=0)\n",
    "        predictions[i, molecules*3:2*molecules*3] = temp\n",
    "\n",
    "        test = np.append(test[:,1:,:], np.expand_dims(prediction, axis=0), axis=1)\n",
    "\n",
    "        if horizon != 1:\n",
    "            if i % horizon != 0:\n",
    "                test = np.append(test[:,1:,:], np.expand_dims(prediction, axis=0), axis=1)\n",
    "\n",
    "            else:\n",
    "                test = np.expand_dims(input_[-steps_to_pred+i+1, :, :], axis=0)\n",
    "        else:\n",
    "            test = np.expand_dims(input_[-steps_to_pred+i+1, :, :], axis=0)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Step %i predicted\" %i)\n",
    "\n",
    "    np.savetxt(\"11performance_%s_horizon_%s.csv\" %(species, horizon), performance, delimiter=\",\")\n",
    "    np.savetxt(\"11predictions_%s_horizon_%s.csv\" %(species, horizon), predictions, delimiter=\",\")\n",
    "\n",
    "#np.savetxt(\"11performance_%s.csv\" %species, performance, delimiter=\",\")\n",
    "#np.savetxt(\"11predictions_%s.csv\" %species, predictions, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bcabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
